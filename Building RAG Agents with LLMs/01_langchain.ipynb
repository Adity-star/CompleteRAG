{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYBNuTdpberE",
        "outputId": "16303548-08e9-4a17-889a-e0d4e1aaf824"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.3/51.3 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m103.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        " %pip install -q langchain langchain-nvidia-ai-endpoints gradio langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install langserve"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltVazrF-gE57",
        "outputId": "e1113181-244f-4820-d7b7-de2e62384e95"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langserve\n",
            "  Downloading langserve-0.3.1-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/40.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httpx<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langserve) (0.28.1)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.3 in /usr/local/lib/python3.11/dist-packages (from langserve) (0.3.47)\n",
            "Requirement already satisfied: orjson<4,>=2 in /usr/local/lib/python3.11/dist-packages (from langserve) (3.10.15)\n",
            "Requirement already satisfied: pydantic<3.0,>=2.7 in /usr/local/lib/python3.11/dist-packages (from langserve) (2.10.6)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0,>=0.23.0->langserve) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0,>=0.23.0->langserve) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0,>=0.23.0->langserve) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1.0,>=0.23.0->langserve) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0,>=0.23.0->langserve) (0.14.0)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.3->langserve) (0.3.18)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.3->langserve) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.3->langserve) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.3->langserve) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.3->langserve) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.3->langserve) (4.12.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0,>=2.7->langserve) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0,>=2.7->langserve) (2.27.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.3->langserve) (3.0.0)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.3->langserve) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.3->langserve) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.3->langserve) (0.23.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1.0,>=0.23.0->langserve) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.3->langserve) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.3->langserve) (2.3.0)\n",
            "Downloading langserve-0.3.1-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langserve\n",
            "Successfully installed langserve-0.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"NVIDIA_API_KEY\"] = \"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\""
      ],
      "metadata": {
        "id": "aUWw_yx2bjjq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Chains and Runnables\n",
        "- `RunnableLambda` -   Used to wrap a function (or a lambda) in such a way that the function can be \"runnable\" in a standardized way, meaning it can be executed as part of a pipeline or series of operations in a larger system.\n",
        "- `RunnablePassThrough` -  It receives input and returns it exactly as it was, without any modifications. This can be useful when you want to include something in a pipeline or sequence of operations but don’t want to alter the data at that step."
      ],
      "metadata": {
        "id": "l1Mg3tyTbjhg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
        "from functools import partial\n",
        "\n",
        "identity = RunnableLambda(lambda x: x)\n",
        "\n",
        "def print_and_return(x, preface=\"\"):\n",
        "    print(f\"{preface}{x}\")\n",
        "    return x\n",
        "\n",
        "rprint0 = RunnableLambda(print_and_return)"
      ],
      "metadata": {
        "id": "G08jgw1GbjfM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## You can also pre-fill some of values using functools.partial\n",
        "rprint1 = RunnableLambda(partial(print_and_return, preface=\"1: \"))\n",
        "\n",
        "## And you can use the same idea to make your own custom Runnable generator\n",
        "def RPrint(preface=\"\"):\n",
        "    return RunnableLambda(partial(print_and_return, preface=preface))\n",
        "\n",
        "## Chaining two runnables\n",
        "chain1 = identity | rprint0\n",
        "chain1.invoke(\"Hello World!\")\n",
        "print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mQyJa-sbjdL",
        "outputId": "e00dab11-4c46-481f-b8e7-6b0b146e9551"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello World!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Chaining that one in as well\n",
        "output = (\n",
        "    chain1           ## Prints \"Welcome Home!\" & passes \"Welcome Home!\" onward\n",
        "    | rprint1        ## Prints \"1: Welcome Home!\" & passes \"Welcome Home!\" onward\n",
        "    | RPrint(\"2: \")  ## Prints \"2: Welcome Home!\" & passes \"Welcome Home!\" onward\n",
        ").invoke(\"Welcome Home!\")\n",
        "\n",
        "## Final Output Is Preserved As \"Welcome Home!\"\n",
        "print(\"\\nOutput:\", output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZUlYbFIc1w3",
        "outputId": "f39081c7-d222-47b9-c500-f373eca0bcd5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome Home!\n",
            "1: Welcome Home!\n",
            "2: Welcome Home!\n",
            "\n",
            "Output: Welcome Home!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Simple LLM Chain"
      ],
      "metadata": {
        "id": "26CHXdHlc4UE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "## Simple Chat Pipeline\n",
        "chat_llm = ChatNVIDIA(model=\"meta/llama3-8b-instruct\")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Only respond in rhymes\"),\n",
        "    (\"user\", \"{input}\")\n",
        "])\n",
        "\n",
        "rhyme_chain = prompt | chat_llm | StrOutputParser()\n",
        "\n",
        "print(rhyme_chain.invoke({\"input\" : \"Tell me about wisdom!\"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UGfS6mqddT_",
        "outputId": "a3a113a6-36af-4d89-b2d7-e7689d60ebb2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wisdom's a treasure so fine,\n",
            "It's a quality that's truly divine.\n",
            "It's gathered with years, or so they say,\n",
            "Through life's experiences, day by day.\n",
            "\n",
            "It's the voice of reason, the guide and the way,\n",
            "To navigate life's ups and downs each new day.\n",
            "It's the skill of discernment, a careful gaze,\n",
            "To see through the noise, and find peaceful ways.\n",
            "\n",
            "So cherish this wisdom, it's a precious find,\n",
            "For it helps us grow, and leaves our fears behind.\n",
            "With an open heart, and a mind so bright,\n",
            "We can shine with wisdom, in the dark of night!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "## Streaming Interface\n",
        "\n",
        "def rhyme_chat_stream(message, history):\n",
        "    buffer = \"\"\n",
        "    for token in rhyme_chain.stream({\"input\" : message}):\n",
        "        buffer += token\n",
        "        yield buffer\n",
        "\n"
      ],
      "metadata": {
        "id": "zMKDmltHddQ4"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Uncomment when you're ready to try this.\n",
        "demo = gr.ChatInterface(rhyme_chat_stream).queue()\n",
        "\n",
        "window_kwargs = {}\n",
        "\n",
        "demo.launch(share=True, debug=True, **window_kwargs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "id": "w3rks_peddOy",
        "outputId": "9faecaba-d6b3-4581-b5e9-2f06a80283f0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gradio/chat_interface.py:334: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  self.chatbot = Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://fbe5ef0665caf34a6d.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://fbe5ef0665caf34a6d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://fbe5ef0665caf34a6d.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Internal Response\n",
        "In order, this zero-shot classification chain:\n",
        "\n",
        "Takes in a dictionary with two required keys, input and options.\n",
        "Passes it through the zero-shot prompt to get the input to our LLM.\n",
        "Passes that string to the model to get the result."
      ],
      "metadata": {
        "id": "KlyQkaX-ddMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "instruct_llm = ChatNVIDIA(model=\"mistralai/mistral-7b-instruct-v0.2\")\n",
        "\n",
        "sys_msg = (\n",
        "    \"Choose the most likely topic classification given the sentence as context.\"\n",
        "    \" Only one word, no explanation.\\n[Options : {options}]\"\n",
        ")\n",
        "\n",
        "## One-shot classification prompt with heavy format assumptions.\n",
        "zsc_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", sys_msg),\n",
        "    (\"user\", \"[[The sea is awesome]]\"),\n",
        "    (\"assistant\", \"boat\"),\n",
        "    (\"user\", \"[[{input}]]\"),\n",
        "])"
      ],
      "metadata": {
        "id": "0aBY36FtddKe"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Roughly equivalent as above for <s>[INST]instruction[/INST]response</s> format\n",
        "zsc_prompt = ChatPromptTemplate.from_template(\n",
        "    f\"{sys_msg}\\n\\n\"\n",
        "    \"[[The sea is awesome]][/INST]boat</s><s>[INST]\"\n",
        "    \"[[{input}]]\"\n",
        ")\n",
        "\n",
        "zsc_chain = zsc_prompt | instruct_llm | StrOutputParser()\n"
      ],
      "metadata": {
        "id": "XgnFw7NCfB7q"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def zsc_call(input, options=[\"car\", \"boat\", \"airplane\", \"bike\"]):\n",
        "    return zsc_chain.invoke({\"input\" : input, \"options\" : options}).split()[0]\n",
        "\n",
        "print(\"-\" * 80)\n",
        "print(zsc_call(\"Should I take the next exit, or keep going to the next one?\"))\n",
        "\n",
        "print(\"-\" * 80)\n",
        "print(zsc_call(\"I get seasick, so I think I'll pass on the trip\"))\n",
        "\n",
        "print(\"-\" * 80)\n",
        "print(zsc_call(\"I'm scared of heights, so flying probably isn't for me\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XdVu7-vfEIm",
        "outputId": "bc87e08e-cc1c-416d-8421-9bac46fc8efb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "car\n",
            "--------------------------------------------------------------------------------\n",
            "boat\n",
            "--------------------------------------------------------------------------------\n",
            "airplane\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Multi-Component Chain"
      ],
      "metadata": {
        "id": "iqJkPXNMfI4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
        "from functools import partial\n",
        "\n",
        "## dictionary enforcement methods\n",
        "def make_dictionary(v, key):\n",
        "    if isinstance(v, dict):\n",
        "        return v\n",
        "    return {key : v}\n",
        "\n",
        "\n",
        "def RInput(key='input'):\n",
        "    '''Coercing method to mold a value (i.e. string) to in-like dict'''\n",
        "    return RunnableLambda(partial(make_dictionary, key=key))\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "W9H01swjfYpi"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ROutput(key='output'):\n",
        "    '''Coercing method to mold a value (i.e. string) to out-like dict'''\n",
        "    return RunnableLambda(partial(make_dictionary, key=key))\n",
        "\n",
        "\n",
        "def RPrint(preface=\"\"):\n",
        "    return RunnableLambda(partial(print_and_return, preface=preface))\n"
      ],
      "metadata": {
        "id": "r61lzflhfYmI"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Common LCEL utility for pulling values from dictionaries\n",
        "from operator import itemgetter\n",
        "\n",
        "up_and_down = (\n",
        "    RPrint(\"A: \")\n",
        "    ## Custom ensure-dictionary process\n",
        "    | RInput()\n",
        "    | RPrint(\"B: \")\n",
        "    ## Pull-values-from-dictionary utility\n",
        "    | itemgetter(\"input\")\n",
        "    | RPrint(\"C: \")\n",
        "    ## Anything-in Dictionary-out implicit map\n",
        "    | {\n",
        "        'word1' : (lambda x : x.split()[0]),\n",
        "        'word2' : (lambda x : x.split()[1]),\n",
        "        'words' : (lambda x: x),  ## <- == to RunnablePassthrough()\n",
        "    }\n",
        "    | RPrint(\"D: \")\n",
        "    | itemgetter(\"word1\")\n",
        "    | RPrint(\"E: \")\n",
        "    ## Anything-in anything-out lambda application\n",
        "    | RunnableLambda(lambda x: x.upper())\n",
        "    | RPrint(\"F: \")\n",
        "    ## Custom ensure-dictionary process\n",
        "    | ROutput()\n",
        ")\n",
        "\n",
        "up_and_down.invoke({\"input\" : \"Hello World\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "344MNLvsfYj5",
        "outputId": "0d77d49e-1aeb-4347-837e-2b282b3b47ab"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A: {'input': 'Hello World'}\n",
            "B: {'input': 'Hello World'}\n",
            "C: Hello World\n",
            "D: {'word1': 'Hello', 'word2': 'World', 'words': 'Hello World'}\n",
            "E: Hello\n",
            "F: HELLO\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'output': 'HELLO'}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## NOTE how the dictionary enforcement methods make it easy to make the following syntax equivalent\n",
        "up_and_down.invoke(\"Hello World\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxzV7vZLfYh4",
        "outputId": "294a021e-9ef0-4350-bede-765d199ebc8d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A: Hello World\n",
            "B: {'input': 'Hello World'}\n",
            "C: Hello World\n",
            "D: {'word1': 'Hello', 'word2': 'World', 'words': 'Hello World'}\n",
            "E: Hello\n",
            "F: HELLO\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'output': 'HELLO'}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Deeper LangChain Integrations"
      ],
      "metadata": {
        "id": "6-pH2I2dflkr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langserve import RemoteRunnable\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
        "\n",
        "# Ensure LangServe is running and accessible at the specified address and port\n",
        "# Verify the address and port match your LangServe instance\n",
        "llm = RemoteRunnable(\"http://0.0.0.0:9012/basic_chat/\") | StrOutputParser()\n",
        "\n",
        "for token in llm.stream(\"Hello World! How is it going?\"):\n",
        "    print(token, end='')"
      ],
      "metadata": {
        "id": "OMGc112rf8NI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vNUvDpb2f8Jo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7qERYJMef8HJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6XpFEldFf8E0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}