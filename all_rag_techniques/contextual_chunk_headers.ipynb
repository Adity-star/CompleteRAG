{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Contextual Chunk Headers (CCH)\n",
        "Contextual chunk headers (CCH) is a method of creating chunk headers that contain higher-level context (such as document-level or section-level context), and prepending those chunk headers to the chunks prior to embedding them. This gives the embeddings a much more accurate and complete representation of the content and meaning of the text. In our testing, this feature leads to a substantial improvement in retrieval quality. In addition to increasing the rate at which the correct information is retrieved, CCH also reduces the rate at which irrelevant results show up in the search results. This reduces the rate at which the LLM misinterprets a piece of text in downstream chat and generation applications."
      ],
      "metadata": {
        "id": "oO6BbBQkzXDJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otQwuV1q--EL"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain-openai python-dotenv langchain-nvidia-ai-endpoints tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "\n",
        "def _set_env(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
        "\n",
        "_set_env(\"NVIDIA_API_KEY\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkJ68exf_TzP",
        "outputId": "6883bc60-06b6-47af-d98a-f792e1c17fbc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NVIDIA_API_KEY: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "\n",
        "llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\")\n",
        "\n",
        "llm.invoke(\"what is capital of India\").content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ZEfrRtQN_aV7",
        "outputId": "0e4424c6-be53-40ee-c3ef-bfe69b1c3ef3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The capital of India is **New Delhi**.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "from typing import List\n",
        "import os\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"COHERE_API_KEY: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcP2CNa9zurT",
        "outputId": "dd374c96-6de3-44de-c741-e192c0d77149"
      },
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "COHERE_API_KEY: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Load the document and split it into chunks"
      ],
      "metadata": {
        "id": "ZJzgyzky07En"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "def split_into_chunks(text: str, chunk_size: int= 800) -> list[str]:\n",
        "  text_splitter = RecursiveCharacterTextSplitter(\n",
        "      chunk_size = chunk_size,\n",
        "      chunk_overlap=0,\n",
        "      length_function=len,\n",
        "  )\n",
        "  documents = text_splitter.create_documents([text])\n",
        "  return [document.page_content for document in documents]\n"
      ],
      "metadata": {
        "id": "KHYI3-vK1CBs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/Text summarization.txt'\n",
        "# Read the document and split it into chunks\n",
        "with open(file_path, \"r\") as file:\n",
        "    document_text = file.read()\n",
        "\n",
        "chunks = split_into_chunks(document_text, chunk_size=800)"
      ],
      "metadata": {
        "id": "VtEYgRN51IyO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Generate descriptive document title to use in chunk header"
      ],
      "metadata": {
        "id": "Y64_fgkm19PJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "DOCUMENT_TITLE_PROMPT = \"\"\"\n",
        "INSTRUCTIONS\n",
        "What is the title of the following document?\n",
        "\n",
        "Your response MUST be the title of the document, and nothing else. DO NOT respond with anything else.\n",
        "\n",
        "{document_title_guidance}\n",
        "\n",
        "{truncation_message}\n",
        "\n",
        "DOCUMENT\n",
        "{document_text}\n",
        "\"\"\".strip()\n",
        "\n",
        "TRUNCATION_MESSAGE = \"\"\"\n",
        "Also note that the document text provided below is just the first ~{num_words} words of the document. That should be plenty for this task. Your response should still pertain to the entire document, not just the text provided below.\n",
        "\"\"\".strip()"
      ],
      "metadata": {
        "id": "BpyzZYK12B0Y"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_CONTENT_TOKENS = 4000\n",
        "MODEL_NAME = \"meta/llama-3.1-70b-instruct\"\n",
        "TOKEN_ENCODER = tiktoken.encoding_for_model(MODEL_NAME)\n",
        "\n",
        "def make_llm_call(chat_message: list[dict]) ->  str:\n",
        "  client = ChatNVIDIA(model=MODEL_NAME)\n",
        "  # The ChatNVIDIA object directly has an invoke method\n",
        "  # Pass the prompt as the 'input' argument\n",
        "  response = client.invoke(\n",
        "      input=chat_message[0]['content'],\n",
        "      max_tokens=MAX_CONTENT_TOKENS,\n",
        "      temperature=0.2\n",
        "\n",
        "  )\n",
        "  return response.content.strip() # Access the content of the response# Access the content of the response\n",
        "\n",
        "def truncate_content(content: str, max_tokens: int) -> tuple[str, int]:\n",
        "    tokens = TOKEN_ENCODER.encode(content, disallowed_special=())\n",
        "    truncated_tokens = tokens[:max_tokens]\n",
        "    return TOKEN_ENCODER.decode(truncated_tokens), min(len(tokens), max_tokens)\n",
        "\n",
        "def get_document_title(document_text: str, document_title_guidance: str = \"\") -> str:\n",
        "   # Truncate the content if it's too long\n",
        "    document_text, num_tokens = truncate_content(document_text, MAX_CONTENT_TOKENS)\n",
        "    truncation_message = TRUNCATION_MESSAGE.format(num_words=3000) if num_tokens >= MAX_CONTENT_TOKENS else \"\"\n",
        "\n",
        "    # Prepare the prompt for title extraction\n",
        "    prompt = DOCUMENT_TITLE_PROMPT.format(\n",
        "        document_title_guidance=document_title_guidance,\n",
        "        document_text=document_text,\n",
        "        truncation_message=truncation_message\n",
        "    )\n",
        "    chat_messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "    return make_llm_call(chat_messages)\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Assuming document_text is defined elsewhere\n",
        "    document_title = get_document_title(document_text)\n",
        "    print(f\"Document Title: {document_title}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Va1pcEL2ByP",
        "outputId": "5059024b-e3ac-4953-f812-a94401e1e6de"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document Title: Text Summarization Assistant\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Add chunk header and measure impact"
      ],
      "metadata": {
        "id": "tJtc6zSt2BwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "COHERE_API_KEY = \"gdCnMqRm4iW5lqhsnCSWITy3zaapGV5KjlT6TukH\""
      ],
      "metadata": {
        "id": "CzW3ZiqJ4ha9"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cohere\n",
        "def rerank_documents(query: str, chunks: List[str]) -> List[float]:\n",
        "    MODEL = \"rerank-english-v3.0\"\n",
        "    client = cohere.Client(api_key=os.environ[\"COHERE_API_KEY\"])\n",
        "\n",
        "    reranked_results = client.rerank(model=MODEL, query=query, documents=chunks)\n",
        "    results = reranked_results.results\n",
        "    reranked_indices = [result.index for result in results]\n",
        "    reranked_similarity_scores = [result.relevance_score for result in results]\n",
        "\n",
        "    # Convert back to order of original documents\n",
        "    similarity_scores = [0] * len(chunks)\n",
        "    for i, index in enumerate(reranked_indices):\n",
        "        similarity_scores[index] = reranked_similarity_scores[i]\n",
        "\n",
        "    return similarity_scores\n",
        "\n",
        "def compare_chunk_similarities(chunk_index: int, chunks: List[str], document_title: str, query: str) -> None:\n",
        "    chunk_text = chunks[chunk_index]\n",
        "    chunk_wo_header = chunk_text\n",
        "    chunk_w_header = f\"Document Title: {document_title}\\n\\n{chunk_text}\"\n",
        "\n",
        "    similarity_scores = rerank_documents(query, [chunk_wo_header, chunk_w_header])\n",
        "\n",
        "    print(f\"\\nChunk header:\\nDocument Title: {document_title}\")\n",
        "    print(f\"\\nChunk text:\\n{chunk_text}\")\n",
        "    print(f\"\\nQuery: {query}\")\n",
        "    print(f\"\\nSimilarity without contextual chunk header: {similarity_scores[0]:.4f}\")\n",
        "    print(f\"Similarity with contextual chunk header: {similarity_scores[1]:.4f}\")\n",
        "\n",
        "CHUNK_INDEX_TO_INSPECT = 2 # Change to a valid index within the range of chunks list\n",
        "QUERY = \"Text summarization method\"\n",
        "\n",
        "compare_chunk_similarities(CHUNK_INDEX_TO_INSPECT, chunks, document_title, QUERY)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_oxN9-Z2Bt0",
        "outputId": "075e1617-c8e1-41b2-8322-9e21640efe0a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chunk header:\n",
            "Document Title: Text Summarization Assistant\n",
            "\n",
            "Chunk text:\n",
            "Target Users:\n",
            "Project managers, analysts, and professionals who need quick insights from large volumes of text.\n",
            "\n",
            "Query: Text summarization method\n",
            "\n",
            "Similarity without contextual chunk header: 0.0107\n",
            "Similarity with contextual chunk header: 0.9587\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UVXCYZKP2Bm5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}