{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install -q langchain langchain-nvidia-ai-endpoints gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-x3Rv2E3ibZ",
        "outputId": "48e2e06e-9161-4e69-a320-81b0fd4063cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"NVIDIA_API_KEY\"] = \"YOUR_API_KEY\"\n"
      ],
      "metadata": {
        "id": "28-DXXwP3l2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rhyme Re-themer Chatbot\n",
        "A simple poetry generationchatbot  that showcases how you might organize two different tasks under the guise of a single agent. The system calls back to the simple Gradio example, but extends it with some boiler-plate responses and logic behind the scenes.\n",
        "\n",
        "It's primary feature is as follows:\n",
        "\n",
        "* On the first response, it will generate a poem based on your response.\n",
        "* On subsequent responses, it will keep the format and structure of your original rhyme while modifying the topic of the poem."
      ],
      "metadata": {
        "id": "lxxyzlN62-52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "\n",
        "[model for model in ChatNVIDIA.get_available_models()\n",
        "     if (\"mistral\" in model.id or \"meta/llama\" in model.id)\n",
        "         and model.model_type in ('chat', None)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jODzUdqu3WR7",
        "outputId": "9eac5b76-d65c-46f0-c768-9648c0f80b03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Model(id='mistralai/mixtral-8x22b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mixtral-8x22b-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='mistralai/mistral-large-2-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
              " Model(id='mistralai/mistral-large', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mistral-large'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='mistralai/mistral-7b-instruct-v0.3', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mistral-7b-instruct-v03'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='mistralai/mathstral-7b-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='meta/llama-3.2-1b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, base_model=None),\n",
              " Model(id='mistralai/mamba-codestral-7b-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='mistralai/codestral-22b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-codestral-22b-instruct-v01'], supports_tools=False, supports_structured_output=True, base_model=None),\n",
              " Model(id='meta/llama-3.3-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
              " Model(id='meta/llama-3.1-405b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
              " Model(id='meta/llama3-8b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-llama3-8b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='nvidia/mistral-nemo-minitron-8b-8k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, base_model=None),\n",
              " Model(id='meta/llama3-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-llama3-70b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='meta/llama-3.1-8b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
              " Model(id='meta/llama-3.2-3b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
              " Model(id='mistralai/mistral-7b-instruct-v0.2', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mistral-7b-instruct-v2', 'playground_mistral_7b', 'mistral_7b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='nv-mistralai/mistral-nemo-12b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
              " Model(id='mistralai/mixtral-8x7b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mixtral-8x7b-instruct', 'playground_mixtral_8x7b', 'mixtral_8x7b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='meta/llama2-70b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-llama2-70b', 'playground_llama2_70b', 'llama2_70b', 'playground_llama2_13b', 'llama2_13b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
              " Model(id='meta/llama-3.1-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None)]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from copy import deepcopy\n",
        "\n",
        "model = ChatNVIDIA(model_id=\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
        "\n",
        "prompt_1 = ChatPromptTemplate.from_messages([(\"user\", (\n",
        "    \"INSTRUCTION: Only respond in rhymes\"\n",
        "    \"\\n\\nPROMPT: {input}\"\n",
        "))])"
      ],
      "metadata": {
        "id": "l4uxA-nP3ca7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_2 =  ChatPromptTemplate.from_messages([(\"user\", (\n",
        "    \"INSTRUCTION: Only responding in rhyme, change the topic of the input poem to be about {topic}!\"\n",
        "    \" Make it happy! Try to keep the same sentence structure, but make sure it's easy to recite!\"\n",
        "    \" Try not to rhyme a word with itself.\"\n",
        "    \"\\n\\nOriginal Poem: {input}\"\n",
        "    \"\\n\\nNew Topic: {topic}\"\n",
        "))])"
      ],
      "metadata": {
        "id": "nDH9hUkC4Z8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# chain 1 which only excects input\n",
        "chain_1 = prompt_1 | model | StrOutputParser()\n",
        "\n",
        "# excepts both input and topic\n",
        "chain_2 = prompt_2 | model | StrOutputParser()"
      ],
      "metadata": {
        "id": "v87b1l--4lGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rhyme_chat_stream(message, history,return_buffer=True):\n",
        "    '''This is a generator function, where each call will yield the next entry'''\n",
        "\n",
        "    first_poem = None\n",
        "    for entry in history:\n",
        "      if entry[0] and entry[1]:\n",
        "        first_poem = '\\n\\n'.join(entry[1].split('\\n\\n')[1:-1])\n",
        "        break\n",
        "\n",
        "      if first_poem is None:\n",
        "        ## First Case: There is no initial poem generated. Better make one up!\n",
        "\n",
        "        buffer = \"Oh! I can make a wonderful poem about that! Let me think!\\n\\n\"\n",
        "        yield buffer\n",
        "\n",
        "         ## iterate over stream generator for first generation\n",
        "        inst_out = \"\"\n",
        "        chat_gen = chain_1.stream({\"input\" : message})\n",
        "        for token in chat_gen:\n",
        "            inst_out += token\n",
        "            buffer += token\n",
        "            yield buffer if return_buffer else token\n",
        "\n",
        "        passage = \"\\n\\nNow let me rewrite it with a different focus! What should the new focus be?\"\n",
        "        buffer += passage\n",
        "        yield buffer if return_buffer else passage\n",
        "    else:\n",
        "        ## Subsequent Cases: There is a poem to start with. Generate a similar one with a new topic!\n",
        "\n",
        "        # yield f\"Not Implemented!!!\"; return ## <- TODO: Comment this out\n",
        "\n",
        "        ########################################################################\n",
        "        ## TODO: Invoke the second chain to generate the new rhymes.\n",
        "\n",
        "        buffer = f\"Sure! Here you go!\\n\\n\"\n",
        "        yield buffer\n",
        "\n",
        "         ## iterate over stream generator for second generation\n",
        "        chat_gen = chain_2.stream({\"input\" : first_poem, \"topic\" : message})\n",
        "        for token in chat_gen:\n",
        "            buffer += token\n",
        "            yield buffer if return_buffer else token\n",
        "\n",
        "        ## END TODO\n",
        "        ########################################################################\n",
        "\n",
        "        passage = \"\\n\\nThis is fun! Give me another topic!\"\n",
        "        buffer += passage\n",
        "        yield buffer if return_buffer else passage\n"
      ],
      "metadata": {
        "id": "XNpZUtUx41xZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Below: This is a small-scale simulation of the gradio routine.\n",
        "\n",
        "def queue_fake_streaming_gradio(chat_stream, history = [], max_questions=3):\n",
        "\n",
        "    ## Mimic of the gradio initialization routine, where a set of starter messages can be printed off\n",
        "    for human_msg, agent_msg in history:\n",
        "        if human_msg: print(\"\\n[ Human ]:\", human_msg)\n",
        "        if agent_msg: print(\"\\n[ Agent ]:\", agent_msg)\n",
        "\n",
        "    ## Mimic of the gradio loop with an initial message from the agent.\n",
        "    for _ in range(max_questions):\n",
        "        message = input(\"\\n[ Human ]: \")\n",
        "        print(\"\\n[ Agent ]: \")\n",
        "        history_entry = [message, \"\"]\n",
        "        for token in chat_stream(message, history, return_buffer=False):\n",
        "            print(token, end='')\n",
        "            history_entry[1] += token\n",
        "        history += [history_entry]\n",
        "        print(\"\\n\")"
      ],
      "metadata": {
        "id": "qprvzt0b545G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## history is of format [[User response 0, Bot response 0], ...]\n",
        "history = [[None, \"Let me help you make a poem! What would you like for me to write?\"]]\n",
        "\n",
        "## Simulating the queueing of a streaming gradio interface, using python input\n",
        "queue_fake_streaming_gradio(\n",
        "    chat_stream = rhyme_chat_stream,\n",
        "    history = history\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6qefUX56AIG",
        "outputId": "3ccd39d3-b414-42ce-b3f8-8e28036101ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[ Agent ]: Let me help you make a poem! What would you like for me to write?\n",
            "\n",
            "[ Human ]: god\n",
            "\n",
            "[ Agent ]: \n",
            "Oh! I can make a wonderful poem about that! Let me think!\n",
            "\n",
            "What a topic so grand and so bold,\n",
            "God is a wonder that's worth being told.\n",
            "His power and wisdom shine so bright and clear,\n",
            "Filling our hearts with love and banishing fear.\n",
            "\n",
            "Now let me rewrite it with a different focus! What should the new focus be?Sure! Here you go!\n",
            "\n",
            "Let's create a happy poem about God!\n",
            "\n",
            "Please input the empty poem, and I'll get started!\n",
            "\n",
            "This is fun! Give me another topic!\n",
            "\n",
            "\n",
            "[ Human ]: stop\n",
            "\n",
            "[ Agent ]: \n",
            "Oh! I can make a wonderful poem about that! Let me think!\n",
            "\n",
            "Hop!\n",
            "\n",
            "Now let me rewrite it with a different focus! What should the new focus be?\n",
            "\n",
            "\n",
            "[ Human ]: stop\n",
            "\n",
            "[ Agent ]: \n",
            "Oh! I can make a wonderful poem about that! Let me think!\n",
            "\n",
            "Hop!\n",
            "\n",
            "Now let me rewrite it with a different focus! What should the new focus be?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "## Simple way to initialize history for the ChatInterface\n",
        "chatbot = gr.Chatbot(value = [[None, \"Let me help you make a poem! What would you like for me to write?\"]])\n",
        "\n",
        "## IF USING COLAB: Share=False is faster\n",
        "gr.ChatInterface(rhyme_chat_stream, chatbot=chatbot).queue().launch(debug=True, share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 722
        },
        "id": "PQDVaTJU6CUr",
        "outputId": "8f93ebc6-9a53-4cd3-8092-a7e35b01951b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gradio/chat_interface.py:95: UserWarning: The function to ChatInterface should take two inputs (message, history) and return a single string response.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "IMPORTANT: You are using gradio version 3.38.0, however version 4.44.1 is available, please upgrade.\n",
            "--------\n",
            "Running on public URL: https://f835728980c9828b08.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f835728980c9828b08.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://f835728980c9828b08.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RrZcjdz-6Q6n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}