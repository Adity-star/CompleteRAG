{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "098bVvENqdey"
      },
      "source": [
        "# Overview\n",
        "-  This notebook demonstrates the integration of Language Models with external Knowledge Bases,\n",
        "-  showcasing how information retrieval (via TF-IDF and cosine similarity) can be used to augment\n",
        "- the responses generated by LLMs, simulating a basic RAG pipeline.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBEkvKV2qqYH"
      },
      "source": [
        "# Process\n",
        "1. **Language Model Query**:\n",
        "- First how to query an OpenAI GPT-3/4 model using a simple prompt.\n",
        "2. **Knowledge Base Creation**:\n",
        "- A sample knowledge base is created using a list of documents.\n",
        "3. **Simple Retrieval Using TF-IDF**:\n",
        "- The knowledge base is indexed using TF-IDF vectors, and a simple cosine similarity search is performed to retrieve the most relevant document for a given query.\n",
        "4. **Simulating RAG**:\n",
        "- simulates a basic Retrieval-Augmented Generation (RAG) pipeline by combining the retrieved knowledge base information with an LLM's response.\n",
        "5. **Expanded Knowledge Base**:\n",
        "- The knowledge base is expanded, and the retrieval system is tested on the larger set of documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gdVf0ejrPIg"
      },
      "source": [
        "# Objective\n",
        "This  notebook gives a foundational understanding of how language models and knowledge bases can be combined, offering a hands-on way to learn about these essential concepts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_oq2odtiaU-",
        "outputId": "b52748fe-873e-4262-ab10-f103bb7c2137"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ],
      "source": [
        "import openai\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import getpass\n",
        "\n",
        "# Set up your OpenAI API key (replace with your actual key)\n",
        "os.environ['OPENAI_API_KEY'] = getpass.getpass()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pux7Cid1i-Fq"
      },
      "source": [
        "## Part 1: Language Models\n",
        "-  A basic introduction to LLMs (using GPT-3 or GPT-4 through OpenAI API)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBY9WZdQo7L0"
      },
      "outputs": [],
      "source": [
        "def query_openai_model(prompt):\n",
        "    \"\"\"\n",
        "    Query OpenAI API (like GPT-3/GPT-4) with a simple prompt.\n",
        "    \"\"\"\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"text-davinci-003\",  # Change based on the model you want to use\n",
        "        prompt=prompt,\n",
        "        max_tokens=100\n",
        "    )\n",
        "    return response.choices[0].text.strip()\n",
        "\n",
        "# Example prompt for GPT-3/GPT-4 model\n",
        "prompt = \"What are the benefits of Retrieval-Augmented Generation (RAG) in AI?\"\n",
        "\n",
        "# Querying GPT Model\n",
        "response = query_openai_model(prompt)\n",
        "print(\"Model's Response to the Prompt: \")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Tsc_cmupw_A"
      },
      "source": [
        "## Part 2: Knowledge Bases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5YOKViapw7Z"
      },
      "outputs": [],
      "source": [
        "# Let's create a simple knowledge base using some predefined text data.\n",
        "knowledge_base = [\n",
        "    \"Retrieval-Augmented Generation (RAG) enhances LLMs by allowing them to access external databases.\",\n",
        "    \"Language models are trained on vast datasets, but they are limited to knowledge available during training.\",\n",
        "    \"RAG systems improve AI responses by retrieving data from external knowledge sources in real-time.\",\n",
        "    \"FAISS is a library for efficient similarity search, commonly used in RAG systems for fast retrieval of relevant documents.\",\n",
        "    \"LangChain is a popular library for building applications with language models and integrating them with external knowledge bases.\"\n",
        "]\n",
        "\n",
        "# Create a simple TF-IDF vectorizer for document indexing in the knowledge base\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(knowledge_base)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACRKGuGfpw46"
      },
      "source": [
        "## Part 3: Querying the Knowledge Base (Simple Retrieval)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9V4KP1Zbpw2l"
      },
      "outputs": [],
      "source": [
        "# Let's create a function to retrieve the most relevant document based on a user's query using cosine similarity.\n",
        "def retrieve_from_knowledge_base(query, knowledge_base, vectorizer):\n",
        "    \"\"\"\n",
        "    Retrieve the most relevant document from the knowledge base based on a query.\n",
        "    \"\"\"\n",
        "    query_vec = vectorizer.transform([query])\n",
        "    similarities = cosine_similarity(query_vec, X)\n",
        "    most_similar_idx = similarities.argmax()\n",
        "    return knowledge_base[most_similar_idx]\n",
        "\n",
        "# Example Query\n",
        "query = \"What is RAG?\"\n",
        "retrieved_document = retrieve_from_knowledge_base(query, knowledge_base, vectorizer)\n",
        "\n",
        "print(\"\\nRetrieved Document Based on Query:\")\n",
        "print(retrieved_document)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2LxxFa1pw0X"
      },
      "source": [
        "## Part 4: Combining LLM and Knowledge Base (Simulating RAG)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Id9r-KIXpwyA"
      },
      "outputs": [],
      "source": [
        "# Let's simulate a basic Retrieval-Augmented Generation approach:\n",
        "def rag_response(query, knowledge_base, vectorizer):\n",
        "    \"\"\"\n",
        "    Simulate a basic Retrieval-Augmented Generation (RAG) response:\n",
        "    Retrieve the relevant document and augment it with LLM response.\n",
        "    \"\"\"\n",
        "    retrieved_doc = retrieve_from_knowledge_base(query, knowledge_base, vectorizer)\n",
        "    augmented_prompt = f\"Query: {query}\\nRelevant Information: {retrieved_doc}\\nAnswer:\"\n",
        "\n",
        "    # Query the OpenAI model with the augmented prompt\n",
        "    model_response = query_openai_model(augmented_prompt)\n",
        "    return model_response\n",
        "\n",
        "# Simulate RAG-based response for a query\n",
        "rag_output = rag_response(\"What is RAG?\", knowledge_base, vectorizer)\n",
        "print(\"\\nRAG-Based Response:\")\n",
        "print(rag_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4ffqKNtqSil"
      },
      "source": [
        "## Part 5: Exploring Knowledge base Expansion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BO0xW-lCqSgP"
      },
      "outputs": [],
      "source": [
        "# Let's extend the knowledge base and see how retrieval and RAG work with a more comprehensive knowledge base.\n",
        "expanded_knowledge_base = knowledge_base + [\n",
        "    \"RAG can be used to build robust question-answering systems.\",\n",
        "    \"Vector search is a powerful method for searching over large-scale data.\",\n",
        "    \"Retrieval-augmented methods are used in a variety of industries like healthcare, finance, and legal.\",\n",
        "    \"RAG models often integrate traditional search engines with language models for more relevant answers.\",\n",
        "]\n",
        "\n",
        "# Update vectorizer and document matrix\n",
        "X_expanded = vectorizer.fit_transform(expanded_knowledge_base)\n",
        "\n",
        "# Query the system again with expanded knowledge base\n",
        "expanded_rag_output = rag_response(\"How can RAG be used in healthcare?\", expanded_knowledge_base, vectorizer)\n",
        "print(\"\\nRAG-Based Response with Expanded Knowledge Base:\")\n",
        "print(expanded_rag_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTNjz1tWqSdy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
