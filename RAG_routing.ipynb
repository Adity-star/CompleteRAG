{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnKMeziMH6MS"
      },
      "outputs": [],
      "source": [
        "! pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain\n",
        "! pip install langchain-google-genai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Environment"
      ],
      "metadata": {
        "id": "-Vc6Q8FkPSDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Langsmith\n",
        "import os\n",
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
        "os.environ['LANGCHAIN_API_KEY'] = \"LANGCHAIN_API_KEY\""
      ],
      "metadata": {
        "id": "k6UM5j6sH-kS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['GOOGLE_API_KEY'] = \"GOOGLE_API_KEY\""
      ],
      "metadata": {
        "id": "-DWs3QpGH-gu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/content/gen-lang-client-0604130497-70fe0d4700a5.json\""
      ],
      "metadata": {
        "id": "pquV_LJPOHbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "api_key = os.getenv(\"AIzaSyAlc9MyZvCsugoa4eKjkHtndPKav8lv2GY\")\n",
        "\n",
        "# Initialize the language model\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-pro\",\n",
        "    temperature=0,\n",
        "    max_tokens=500,\n",
        "    api_key=api_key\n",
        ")\n",
        "\n",
        "# Call the model\n",
        "response = llm.invoke(\"Hello, world!\")\n",
        "print(response.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTu5M_uPMcV2",
        "outputId": "ac367999-5fdf-4a68-fec7-ba30eb7517a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello to you too!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logical and semantic routing\n",
        "* use function calling for classification"
      ],
      "metadata": {
        "id": "WvMCzWcKPUwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Literal\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Data model\n",
        "class RouteQuery(BaseModel):\n",
        "    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
        "\n",
        "    datasource: Literal[\"python_docs\", \"js_docs\", \"golang_docs\"] = Field(\n",
        "        ...,\n",
        "        description=\"Given a user question choose which datasource would be most relevant for answering their question\",\n",
        "    )\n",
        "\n"
      ],
      "metadata": {
        "id": "ivN8zKtYH-cI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM with function call\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-pro\",\n",
        "    temperature=0,\n",
        "    api_key=api_key)\n",
        "\n",
        "structured_llm = llm.with_structured_output(RouteQuery)\n"
      ],
      "metadata": {
        "id": "r4jh5reVQ1aL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt\n",
        "\n",
        "system = \"\"\"you are an expert at routing a user question to its most relevant datasource.\n",
        "\n",
        "Based on programming language the question is rwferring to ,route it to relevant data source\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Define router\n",
        "router = prompt | structured_llm"
      ],
      "metadata": {
        "id": "CzjARsYgRHir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"\"\"Why doesn't the following code work:\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\"human\", \"speak in {language}\"])\n",
        "prompt.invoke(\"french\")\n",
        "\"\"\"\n",
        "\n",
        "result = router.invoke({\"question\": question})"
      ],
      "metadata": {
        "id": "pykYIsCzRu74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrVZbDMBR6-9",
        "outputId": "6e1b5294-4f4b-42f9-bf1e-6b1777d66c8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RouteQuery(datasource='python_docs')"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.datasource"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "MK1VIirXR8tf",
        "outputId": "3a60b8c0-3176-4f6f-90ec-6f8bf1f00b99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'python_docs'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def choose_route(result):\n",
        "    if \"python_docs\" in result.datasource.lower():\n",
        "        return \"chain for python_docs\"\n",
        "    elif \"js_docs\" in result.datasource.lower():\n",
        "        return \"chain for js_docs\"\n",
        "    else:\n",
        "        return \"golang_docs\"\n",
        "\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "full_chain = router | RunnableLambda(choose_route)"
      ],
      "metadata": {
        "id": "VRjjBrloSAfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_chain.invoke({\"question\": question})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "9BaYcWSXSS0E",
        "outputId": "be344304-bfd1-49f9-fab2-992c056587f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'chain for python_docs'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Semantic Routing"
      ],
      "metadata": {
        "id": "l9EmtlSUSg_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.utils.math import cosine_similarity\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "\n",
        "\n",
        "# We will give two prompts for semantic8i\n",
        "\n",
        "physics_template = \"\"\"You are a very smart physics professor. \\\n",
        "You are great at answering questions about physics in a concise and easy to understand manner. \\\n",
        "When you don't know the answer to a question you admit that you don't know.\n",
        "\n",
        "Here is a question:\n",
        "{query}\"\"\"\n",
        "\n",
        "math_template = \"\"\"You are a very good mathematician. You are great at answering math questions. \\\n",
        "You are so good because you are able to break down hard problems into their component parts, \\\n",
        "answer the component parts, and then put them together to answer the broader question.\n",
        "\n",
        "Here is a question:\n",
        "{query}\"\"\""
      ],
      "metadata": {
        "id": "CK-kpEX4SmdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embed the prompts\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "\n",
        "prompt_templates = [physics_template, math_template]\n",
        "prompt_embeddings = embeddings.embed_documents(prompt_templates)"
      ],
      "metadata": {
        "id": "rJs_0ZeISma6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# route questions to prompt\n",
        "\n",
        "def prompt_router(input):\n",
        "   # Embed the input query\n",
        "  query_embedding = embeddings.embed_query(input[\"query\"])\n",
        "\n",
        "  # find similar embeddings to prompt\n",
        "  similarity = cosine_similarity([query_embedding], prompt_embeddings)[0]\n",
        "  #take the first embeddings from most similar ones\n",
        "  most_similar = prompt_templates[similarity.argmax()]\n",
        "  print(f\"Most similar prompt: {most_similar}\")\n",
        "\n",
        "  # Chosen prompt\n",
        "  print(\"Using MATH\" if most_similar == math_template else \"Using PHYSICS\")\n",
        "  return PromptTemplate.from_template(most_similar)\n"
      ],
      "metadata": {
        "id": "2KTzfdLISmYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the chain\n",
        "chain = (\n",
        "    {\"query\":RunnablePassthrough()}\n",
        "    | RunnableLambda(prompt_router)\n",
        "    | ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0, api_key=api_key)\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(chain.invoke(\"what is modern physics\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSam4_PCSmWU",
        "outputId": "3dda54fe-b069-4da0-9daf-ad2219d02ed6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar prompt: You are a very smart physics professor. You are great at answering questions about physics in a concise and easy to understand manner. When you don't know the answer to a question you admit that you don't know.\n",
            "\n",
            "Here is a question:\n",
            "{query}\n",
            "Using PHYSICS\n",
            "Modern physics generally refers to developments in physics starting around the year 1900, with the introduction of Planck's quantum theory and Einstein's theory of relativity.  It contrasts with \"classical physics\" which encompasses Newtonian mechanics, Maxwell's electromagnetism, and thermodynamics developed before 1900.  Essentially, modern physics deals with the very small (quantum mechanics), the very fast (relativity), and the very massive (cosmology and astrophysics).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trace: [Modern Physics](https://smith.langchain.com/o/60da6ebd-4ff3-45dc-a06f-87d423eb5049/projects/p/f2716ee1-d256-4a10-940f-f222ddbef534?timeModel=%7B%22duration%22%3A%227d%22%7D&peek=fd5c044c-320c-416b-95bc-9dea135d6515)"
      ],
      "metadata": {
        "id": "RTKScsU0VttL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Query Construction\n",
        "* For in depth Knowlwdge refer this [langchain documentation blog](https://blog.langchain.dev/query-construction/)\n",
        "* [Enhancing rag based applications accuracy](https://blog.langchain.dev/enhancing-rag-based-applications-accuracy-by-constructing-and-leveraging-knowledge-graphs/)\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "duT0-SrBVbZn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Query constructing for metadata filters"
      ],
      "metadata": {
        "id": "pYUpBvdsWT-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install youtube-transcript-api\n",
        "!pip install pytube"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pL1DjN47X5YK",
        "outputId": "0a404342-6e54-4d35-cf05-12009f02535c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: youtube-transcript-api in /usr/local/lib/python3.11/dist-packages (1.0.2)\n",
            "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from youtube-transcript-api) (0.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from youtube-transcript-api) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->youtube-transcript-api) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->youtube-transcript-api) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->youtube-transcript-api) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->youtube-transcript-api) (2025.1.31)\n",
            "Collecting pytube\n",
            "  Downloading pytube-15.0.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Downloading pytube-15.0.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytube\n",
            "Successfully installed pytube-15.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "docs = WebBaseLoader(\n",
        "    \"https://github.com/langchain-ai/langgraph/tree/main/examples/rag\"\n",
        ").load()\n",
        "\n",
        "docs[0].metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSggUlCuWV6x",
        "outputId": "df29fb84-b628-41c4-8eb9-93f7a31f3545"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'source': 'https://github.com/langchain-ai/langgraph/tree/main/examples/rag',\n",
              " 'title': 'langgraph/examples/rag at main · langchain-ai/langgraph · GitHub',\n",
              " 'description': 'Build resilient language agents as graphs. Contribute to langchain-ai/langgraph development by creating an account on GitHub.',\n",
              " 'language': 'en'}"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "from typing import Literal, Optional, Tuple\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "\n",
        "class quert_search(BaseModel):\n",
        "    \"\"\"Search over a database of github about a software library.\"\"\"\n",
        "\n",
        "    content_search: str = Field(\n",
        "        ...,\n",
        "        description=\"Similarity search query applied to github notebooks.\",\n",
        "    )\n",
        "    title_search: str = Field(\n",
        "        ...,\n",
        "        description=(\n",
        "            \"Alternate version of the content search query to apply to github notebooks. \"\n",
        "            \"Should be succinct and only include key words that could be in a notebooks \"\n",
        "            \"title.\"\n",
        "        ),\n",
        "    )\n",
        "    def pretty_print(self) -> None:\n",
        "        for field in self.__fields__:\n",
        "            if getattr(self, field) is not None and getattr(self, field) != getattr(\n",
        "                self.__fields__[field], \"default\", None\n",
        "            ):\n",
        "                print(f\"{field}: {getattr(self, field)}\")"
      ],
      "metadata": {
        "id": "7G268RoXWV38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt for llm\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "system = \"\"\"You are an expert at converting user questions into database queries. \\\n",
        "You have access to a database of colab notebooks about a software library for building LLM-powered applications. \\\n",
        "Given a question, return a database query optimized to retrieve the most relevant results.\n",
        "\n",
        "If there are acronyms or words you are not familiar with, do not try to rephrase them.\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "R9s_d5aPWV1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# llm\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-pro\",\n",
        "    temperature=0,\n",
        "    api_key=api_key)\n",
        "\n",
        "structured_llm = llm.with_structured_output(quert_search)\n",
        "query_analyzer = prompt | structured_llm"
      ],
      "metadata": {
        "id": "6ZTCVqyOWVzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_analyzer.invoke({\"question\": \"rag from scratch\"}).pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mSgezN5ajAr",
        "outputId": "26dc66b1-b7cc-4921-d1fb-3742f54b01e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content_search: rag from scratch\n",
            "title_search: rag\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_analyzer.invoke(\n",
        "    {\"question\": \"Notebooks on chat langchain published in 2023\"}\n",
        ").pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXtfbohTalRA",
        "outputId": "51406737-1a60-4748-9795-a477c8593593"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content_search: chat langchain published:2023\n",
            "title_search: chat langchain\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_analyzer.invoke(\n",
        "    {\"question\": \"Notebooks that are focused on the topic of chat langchain that are published before 2024\"}\n",
        ").pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gohRWbtNaq64",
        "outputId": "6a910f0a-de1c-40ca-ee30-4894311cbf29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content_search: chat langchain\n",
            "title_search: chat langchain\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_analyzer.invoke(\n",
        "    {\n",
        "        \"question\": \"how to use multi-modal models in an agent, only videos under 5 minutes\"\n",
        "    }\n",
        ").pretty_print()"
      ],
      "metadata": {
        "id": "DjYVE2G7awnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3JOVSAJJazyz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}