{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RP9erxxZQ4YU"
      },
      "source": [
        "# RAG For Document Chunk Retrieval\n",
        "## 1. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWkRwJT1ONZq",
        "outputId": "4a95d466-1456-4d3d-cbc6-a9adf3b3309d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m303.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.20-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.46)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.21 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.21)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.39)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.14)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.0.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.18)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.21->langchain-community) (0.3.7)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.21->langchain-community) (2.10.6)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.21->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.21->langchain-community) (2.27.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
            "Downloading langchain_community-0.3.20-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.20 marshmallow-3.26.1 mypy-extensions-1.0.0 pydantic-settings-2.8.1 python-dotenv-1.0.1 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "%pip install -q langchain langchain-nvidia-ai-endpoints gradio rich\n",
        "%pip install -q arxiv pymupdf faiss-cpu\n",
        "%pip install -U langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xw12jajuOXCN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"NVIDIA_API_KEY\"] = \"NVIDIA_API_KEY\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5AromdEOimY"
      },
      "outputs": [],
      "source": [
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
        "\n",
        "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
        "\n",
        "instruct_llm = ChatNVIDIA(model=\"mistralai/mixtral-8x22b-instruct-v0.1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdbKlmKFOBEz"
      },
      "source": [
        "## 2.  Loading And Chunking Your Documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xU4QEcyqOikc"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import ArxivLoader\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000, chunk_overlap=100,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \";\", \",\", \" \"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21qYGrhTOiiK",
        "outputId": "5639e22a-954e-4261-aa82-926bcb23edab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Documents\n"
          ]
        }
      ],
      "source": [
        "print(\"Loading Documents\")\n",
        "docs = [\n",
        "    ArxivLoader(query=\"1706.03762\").load(),  ## Attention Is All You Need Paper\n",
        "    ArxivLoader(query=\"1810.04805\").load(),  ## BERT Paper\n",
        "    ArxivLoader(query=\"2005.11401\").load(),  ## RAG Paper\n",
        "    ArxivLoader(query=\"2205.00445\").load(),  ## MRKL Paper\n",
        "    ArxivLoader(query=\"2310.06825\").load(),  ## Mistral Paper\n",
        "    ArxivLoader(query=\"2306.05685\").load(),  ## LLM-as-a-Judge\n",
        "    ArxivLoader(query=\"2210.03629\").load(),  ## ReAct Paper\n",
        "    ArxivLoader(query=\"2112.10752\").load(),  ## Latent Stable Diffusion Paper\n",
        "    ArxivLoader(query=\"2103.00020\").load(),  ## CLIP Paper\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a59zRBTkOigU",
        "outputId": "d52123c7-913c-4dca-80ae-9d5ac229ff9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chunking Documents\n"
          ]
        }
      ],
      "source": [
        "for doc in docs:\n",
        "    content = json.dumps(doc[0].page_content)\n",
        "    if \"References\" in content:\n",
        "        doc[0].page_content = content[:content.index(\"References\")]\n",
        "\n",
        "print(\"Chunking Documents\")\n",
        "docs_chunks = [text_splitter.split_documents(doc) for doc in docs]\n",
        "docs_chunks = [[c for c in dchunks if len(c.page_content) > 200] for dchunks in docs_chunks]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxeZ0vLZOid0"
      },
      "outputs": [],
      "source": [
        "## Make some custom Chunks to give big-picture details\n",
        "doc_string = \"Available Documents:\"\n",
        "doc_metadata = []\n",
        "for chunks in docs_chunks:\n",
        "    metadata = getattr(chunks[0], 'metadata', {})\n",
        "    doc_string += \"\\n - \" + metadata.get('Title')\n",
        "    doc_metadata += [str(metadata)]\n",
        "\n",
        "extra_chunks = [doc_string] + doc_metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKaJT951Oib4",
        "outputId": "993678fc-81a9-4c84-d41c-5813932ac973"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('Available Documents:\\n'\n",
            " ' - Attention Is All You Need\\n'\n",
            " ' - BERT: Pre-training of Deep Bidirectional Transformers for Language '\n",
            " 'Understanding\\n'\n",
            " ' - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\\n'\n",
            " ' - MRKL Systems: A modular, neuro-symbolic architecture that combines large '\n",
            " 'language models, external knowledge sources and discrete reasoning\\n'\n",
            " ' - Mistral 7B\\n'\n",
            " ' - Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\\n'\n",
            " ' - ReAct: Synergizing Reasoning and Acting in Language Models\\n'\n",
            " ' - High-Resolution Image Synthesis with Latent Diffusion Models\\n'\n",
            " ' - Learning Transferable Visual Models From Natural Language Supervision')\n",
            "Document 0\n",
            " - # Chunks: 35\n",
            " - Metadata: \n",
            "{'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion '\n",
            "            'Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
            " 'Published': '2023-08-02',\n",
            " 'Summary': 'The dominant sequence transduction models are based on complex '\n",
            "            'recurrent or\\n'\n",
            "            'convolutional neural networks in an encoder-decoder '\n",
            "            'configuration. The best\\n'\n",
            "            'performing models also connect the encoder and decoder through an '\n",
            "            'attention\\n'\n",
            "            'mechanism. We propose a new simple network architecture, the '\n",
            "            'Transformer, based\\n'\n",
            "            'solely on attention mechanisms, dispensing with recurrence and '\n",
            "            'convolutions\\n'\n",
            "            'entirely. Experiments on two machine translation tasks show these '\n",
            "            'models to be\\n'\n",
            "            'superior in quality while being more parallelizable and requiring '\n",
            "            'significantly\\n'\n",
            "            'less time to train. Our model achieves 28.4 BLEU on the WMT 2014\\n'\n",
            "            'English-to-German translation task, improving over the existing '\n",
            "            'best results,\\n'\n",
            "            'including ensembles by over 2 BLEU. On the WMT 2014 '\n",
            "            'English-to-French\\n'\n",
            "            'translation task, our model establishes a new single-model '\n",
            "            'state-of-the-art\\n'\n",
            "            'BLEU score of 41.8 after training for 3.5 days on eight GPUs, a '\n",
            "            'small fraction\\n'\n",
            "            'of the training costs of the best models from the literature. We '\n",
            "            'show that the\\n'\n",
            "            'Transformer generalizes well to other tasks by applying it '\n",
            "            'successfully to\\n'\n",
            "            'English constituency parsing both with large and limited training '\n",
            "            'data.',\n",
            " 'Title': 'Attention Is All You Need'}\n",
            "\n",
            "Document 1\n",
            " - # Chunks: 45\n",
            " - Metadata: \n",
            "{'Authors': 'Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova',\n",
            " 'Published': '2019-05-24',\n",
            " 'Summary': 'We introduce a new language representation model called BERT, '\n",
            "            'which stands\\n'\n",
            "            'for Bidirectional Encoder Representations from Transformers. '\n",
            "            'Unlike recent\\n'\n",
            "            'language representation models, BERT is designed to pre-train '\n",
            "            'deep\\n'\n",
            "            'bidirectional representations from unlabeled text by jointly '\n",
            "            'conditioning on\\n'\n",
            "            'both left and right context in all layers. As a result, the '\n",
            "            'pre-trained BERT\\n'\n",
            "            'model can be fine-tuned with just one additional output layer to '\n",
            "            'create\\n'\n",
            "            'state-of-the-art models for a wide range of tasks, such as '\n",
            "            'question answering\\n'\n",
            "            'and language inference, without substantial task-specific '\n",
            "            'architecture\\n'\n",
            "            'modifications.\\n'\n",
            "            '  BERT is conceptually simple and empirically powerful. It '\n",
            "            'obtains new\\n'\n",
            "            'state-of-the-art results on eleven natural language processing '\n",
            "            'tasks, including\\n'\n",
            "            'pushing the GLUE score to 80.5% (7.7% point absolute '\n",
            "            'improvement), MultiNLI\\n'\n",
            "            'accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 '\n",
            "            'question answering\\n'\n",
            "            'Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 '\n",
            "            'Test F1 to 83.1\\n'\n",
            "            '(5.1 point absolute improvement).',\n",
            " 'Title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language '\n",
            "          'Understanding'}\n",
            "\n",
            "Document 2\n",
            " - # Chunks: 46\n",
            " - Metadata: \n",
            "{'Authors': 'Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, '\n",
            "            'Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, '\n",
            "            'Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela',\n",
            " 'Published': '2021-04-12',\n",
            " 'Summary': 'Large pre-trained language models have been shown to store '\n",
            "            'factual knowledge\\n'\n",
            "            'in their parameters, and achieve state-of-the-art results when '\n",
            "            'fine-tuned on\\n'\n",
            "            'downstream NLP tasks. However, their ability to access and '\n",
            "            'precisely manipulate\\n'\n",
            "            'knowledge is still limited, and hence on knowledge-intensive '\n",
            "            'tasks, their\\n'\n",
            "            'performance lags behind task-specific architectures. '\n",
            "            'Additionally, providing\\n'\n",
            "            'provenance for their decisions and updating their world knowledge '\n",
            "            'remain open\\n'\n",
            "            'research problems. Pre-trained models with a differentiable '\n",
            "            'access mechanism to\\n'\n",
            "            'explicit non-parametric memory can overcome this issue, but have '\n",
            "            'so far been\\n'\n",
            "            'only investigated for extractive downstream tasks. We explore a '\n",
            "            'general-purpose\\n'\n",
            "            'fine-tuning recipe for retrieval-augmented generation (RAG) -- '\n",
            "            'models which\\n'\n",
            "            'combine pre-trained parametric and non-parametric memory for '\n",
            "            'language\\n'\n",
            "            'generation. We introduce RAG models where the parametric memory '\n",
            "            'is a\\n'\n",
            "            'pre-trained seq2seq model and the non-parametric memory is a '\n",
            "            'dense vector index\\n'\n",
            "            'of Wikipedia, accessed with a pre-trained neural retriever. We '\n",
            "            'compare two RAG\\n'\n",
            "            'formulations, one which conditions on the same retrieved passages '\n",
            "            'across the\\n'\n",
            "            'whole generated sequence, the other can use different passages '\n",
            "            'per token. We\\n'\n",
            "            'fine-tune and evaluate our models on a wide range of '\n",
            "            'knowledge-intensive NLP\\n'\n",
            "            'tasks and set the state-of-the-art on three open domain QA tasks, '\n",
            "            'outperforming\\n'\n",
            "            'parametric seq2seq models and task-specific retrieve-and-extract '\n",
            "            'architectures.\\n'\n",
            "            'For language generation tasks, we find that RAG models generate '\n",
            "            'more specific,\\n'\n",
            "            'diverse and factual language than a state-of-the-art '\n",
            "            'parametric-only seq2seq\\n'\n",
            "            'baseline.',\n",
            " 'Title': 'Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks'}\n",
            "\n",
            "Document 3\n",
            " - # Chunks: 40\n",
            " - Metadata: \n",
            "{'Authors': 'Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher '\n",
            "            'Lieber, Nir Ratner, Yoav Shoham, Hofit Bata, Yoav Levine, Kevin '\n",
            "            'Leyton-Brown, Dor Muhlgay, Noam Rozen, Erez Schwartz, Gal '\n",
            "            'Shachaf, Shai Shalev-Shwartz, Amnon Shashua, Moshe Tenenholtz',\n",
            " 'Published': '2022-05-01',\n",
            " 'Summary': 'Huge language models (LMs) have ushered in a new era for AI, '\n",
            "            'serving as a\\n'\n",
            "            'gateway to natural-language-based knowledge tasks. Although an '\n",
            "            'essential\\n'\n",
            "            'element of modern AI, LMs are also inherently limited in a number '\n",
            "            'of ways. We\\n'\n",
            "            'discuss these limitations and how they can be avoided by adopting '\n",
            "            'a systems\\n'\n",
            "            'approach. Conceptualizing the challenge as one that involves '\n",
            "            'knowledge and\\n'\n",
            "            'reasoning in addition to linguistic processing, we define a '\n",
            "            'flexible\\n'\n",
            "            'architecture with multiple neural models, complemented by '\n",
            "            'discrete knowledge\\n'\n",
            "            'and reasoning modules. We describe this neuro-symbolic '\n",
            "            'architecture, dubbed the\\n'\n",
            "            'Modular Reasoning, Knowledge and Language (MRKL, pronounced '\n",
            "            '\"miracle\") system,\\n'\n",
            "            'some of the technical challenges in implementing it, and '\n",
            "            \"Jurassic-X, AI21 Labs'\\n\"\n",
            "            'MRKL system implementation.',\n",
            " 'Title': 'MRKL Systems: A modular, neuro-symbolic architecture that combines '\n",
            "          'large language models, external knowledge sources and discrete '\n",
            "          'reasoning'}\n",
            "\n",
            "Document 4\n",
            " - # Chunks: 21\n",
            " - Metadata: \n",
            "{'Authors': 'Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris '\n",
            "            'Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian '\n",
            "            'Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, '\n",
            "            'Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le '\n",
            "            'Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El '\n",
            "            'Sayed',\n",
            " 'Published': '2023-10-10',\n",
            " 'Summary': 'We introduce Mistral 7B v0.1, a 7-billion-parameter language '\n",
            "            'model engineered\\n'\n",
            "            'for superior performance and efficiency. Mistral 7B outperforms '\n",
            "            'Llama 2 13B\\n'\n",
            "            'across all evaluated benchmarks, and Llama 1 34B in reasoning, '\n",
            "            'mathematics, and\\n'\n",
            "            'code generation. Our model leverages grouped-query attention '\n",
            "            '(GQA) for faster\\n'\n",
            "            'inference, coupled with sliding window attention (SWA) to '\n",
            "            'effectively handle\\n'\n",
            "            'sequences of arbitrary length with a reduced inference cost. We '\n",
            "            'also provide a\\n'\n",
            "            'model fine-tuned to follow instructions, Mistral 7B -- Instruct, '\n",
            "            'that surpasses\\n'\n",
            "            'the Llama 2 13B -- Chat model both on human and automated '\n",
            "            'benchmarks. Our\\n'\n",
            "            'models are released under the Apache 2.0 license.',\n",
            " 'Title': 'Mistral 7B'}\n",
            "\n",
            "Document 5\n",
            " - # Chunks: 44\n",
            " - Metadata: \n",
            "{'Authors': 'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, '\n",
            "            'Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric '\n",
            "            'P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica',\n",
            " 'Published': '2023-12-24',\n",
            " 'Summary': 'Evaluating large language model (LLM) based chat assistants is '\n",
            "            'challenging\\n'\n",
            "            'due to their broad capabilities and the inadequacy of existing '\n",
            "            'benchmarks in\\n'\n",
            "            'measuring human preferences. To address this, we explore using '\n",
            "            'strong LLMs as\\n'\n",
            "            'judges to evaluate these models on more open-ended questions. We '\n",
            "            'examine the\\n'\n",
            "            'usage and limitations of LLM-as-a-judge, including position, '\n",
            "            'verbosity, and\\n'\n",
            "            'self-enhancement biases, as well as limited reasoning ability, '\n",
            "            'and propose\\n'\n",
            "            'solutions to mitigate some of them. We then verify the agreement '\n",
            "            'between LLM\\n'\n",
            "            'judges and human preferences by introducing two benchmarks: '\n",
            "            'MT-bench, a\\n'\n",
            "            'multi-turn question set; and Chatbot Arena, a crowdsourced battle '\n",
            "            'platform. Our\\n'\n",
            "            'results reveal that strong LLM judges like GPT-4 can match both '\n",
            "            'controlled and\\n'\n",
            "            'crowdsourced human preferences well, achieving over 80% '\n",
            "            'agreement, the same\\n'\n",
            "            'level of agreement between humans. Hence, LLM-as-a-judge is a '\n",
            "            'scalable and\\n'\n",
            "            'explainable way to approximate human preferences, which are '\n",
            "            'otherwise very\\n'\n",
            "            'expensive to obtain. Additionally, we show our benchmark and '\n",
            "            'traditional\\n'\n",
            "            'benchmarks complement each other by evaluating several variants '\n",
            "            'of LLaMA and\\n'\n",
            "            'Vicuna. The MT-bench questions, 3K expert votes, and 30K '\n",
            "            'conversations with\\n'\n",
            "            'human preferences are publicly available at\\n'\n",
            "            'https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.',\n",
            " 'Title': 'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena'}\n",
            "\n",
            "Document 6\n",
            " - # Chunks: 123\n",
            " - Metadata: \n",
            "{'Authors': 'Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik '\n",
            "            'Narasimhan, Yuan Cao',\n",
            " 'Published': '2023-03-10',\n",
            " 'Summary': 'While large language models (LLMs) have demonstrated impressive '\n",
            "            'capabilities\\n'\n",
            "            'across tasks in language understanding and interactive decision '\n",
            "            'making, their\\n'\n",
            "            'abilities for reasoning (e.g. chain-of-thought prompting) and '\n",
            "            'acting (e.g.\\n'\n",
            "            'action plan generation) have primarily been studied as separate '\n",
            "            'topics. In this\\n'\n",
            "            'paper, we explore the use of LLMs to generate both reasoning '\n",
            "            'traces and\\n'\n",
            "            'task-specific actions in an interleaved manner, allowing for '\n",
            "            'greater synergy\\n'\n",
            "            'between the two: reasoning traces help the model induce, track, '\n",
            "            'and update\\n'\n",
            "            'action plans as well as handle exceptions, while actions allow it '\n",
            "            'to interface\\n'\n",
            "            'with external sources, such as knowledge bases or environments, '\n",
            "            'to gather\\n'\n",
            "            'additional information. We apply our approach, named ReAct, to a '\n",
            "            'diverse set of\\n'\n",
            "            'language and decision making tasks and demonstrate its '\n",
            "            'effectiveness over\\n'\n",
            "            'state-of-the-art baselines, as well as improved human '\n",
            "            'interpretability and\\n'\n",
            "            'trustworthiness over methods without reasoning or acting '\n",
            "            'components.\\n'\n",
            "            'Concretely, on question answering (HotpotQA) and fact '\n",
            "            'verification (Fever),\\n'\n",
            "            'ReAct overcomes issues of hallucination and error propagation '\n",
            "            'prevalent in\\n'\n",
            "            'chain-of-thought reasoning by interacting with a simple Wikipedia '\n",
            "            'API, and\\n'\n",
            "            'generates human-like task-solving trajectories that are more '\n",
            "            'interpretable than\\n'\n",
            "            'baselines without reasoning traces. On two interactive decision '\n",
            "            'making\\n'\n",
            "            'benchmarks (ALFWorld and WebShop), ReAct outperforms imitation '\n",
            "            'and\\n'\n",
            "            'reinforcement learning methods by an absolute success rate of 34% '\n",
            "            'and 10%\\n'\n",
            "            'respectively, while being prompted with only one or two '\n",
            "            'in-context examples.\\n'\n",
            "            'Project site with code: https://react-lm.github.io',\n",
            " 'Title': 'ReAct: Synergizing Reasoning and Acting in Language Models'}\n",
            "\n",
            "Document 7\n",
            " - # Chunks: 52\n",
            " - Metadata: \n",
            "{'Authors': 'Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, '\n",
            "            'Björn Ommer',\n",
            " 'Published': '2022-04-13',\n",
            " 'Summary': 'By decomposing the image formation process into a sequential '\n",
            "            'application of\\n'\n",
            "            'denoising autoencoders, diffusion models (DMs) achieve '\n",
            "            'state-of-the-art\\n'\n",
            "            'synthesis results on image data and beyond. Additionally, their '\n",
            "            'formulation\\n'\n",
            "            'allows for a guiding mechanism to control the image generation '\n",
            "            'process without\\n'\n",
            "            'retraining. However, since these models typically operate '\n",
            "            'directly in pixel\\n'\n",
            "            'space, optimization of powerful DMs often consumes hundreds of '\n",
            "            'GPU days and\\n'\n",
            "            'inference is expensive due to sequential evaluations. To enable '\n",
            "            'DM training on\\n'\n",
            "            'limited computational resources while retaining their quality and '\n",
            "            'flexibility,\\n'\n",
            "            'we apply them in the latent space of powerful pretrained '\n",
            "            'autoencoders. In\\n'\n",
            "            'contrast to previous work, training diffusion models on such a '\n",
            "            'representation\\n'\n",
            "            'allows for the first time to reach a near-optimal point between '\n",
            "            'complexity\\n'\n",
            "            'reduction and detail preservation, greatly boosting visual '\n",
            "            'fidelity. By\\n'\n",
            "            'introducing cross-attention layers into the model architecture, '\n",
            "            'we turn\\n'\n",
            "            'diffusion models into powerful and flexible generators for '\n",
            "            'general conditioning\\n'\n",
            "            'inputs such as text or bounding boxes and high-resolution '\n",
            "            'synthesis becomes\\n'\n",
            "            'possible in a convolutional manner. Our latent diffusion models '\n",
            "            '(LDMs) achieve\\n'\n",
            "            'a new state of the art for image inpainting and highly '\n",
            "            'competitive performance\\n'\n",
            "            'on various tasks, including unconditional image generation, '\n",
            "            'semantic scene\\n'\n",
            "            'synthesis, and super-resolution, while significantly reducing '\n",
            "            'computational\\n'\n",
            "            'requirements compared to pixel-based DMs. Code is available at\\n'\n",
            "            'https://github.com/CompVis/latent-diffusion .',\n",
            " 'Title': 'High-Resolution Image Synthesis with Latent Diffusion Models'}\n",
            "\n",
            "Document 8\n",
            " - # Chunks: 155\n",
            " - Metadata: \n",
            "{'Authors': 'Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, '\n",
            "            'Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, '\n",
            "            'Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever',\n",
            " 'Published': '2021-02-26',\n",
            " 'Summary': 'State-of-the-art computer vision systems are trained to predict a '\n",
            "            'fixed set\\n'\n",
            "            'of predetermined object categories. This restricted form of '\n",
            "            'supervision limits\\n'\n",
            "            'their generality and usability since additional labeled data is '\n",
            "            'needed to\\n'\n",
            "            'specify any other visual concept. Learning directly from raw text '\n",
            "            'about images\\n'\n",
            "            'is a promising alternative which leverages a much broader source '\n",
            "            'of\\n'\n",
            "            'supervision. We demonstrate that the simple pre-training task of '\n",
            "            'predicting\\n'\n",
            "            'which caption goes with which image is an efficient and scalable '\n",
            "            'way to learn\\n'\n",
            "            'SOTA image representations from scratch on a dataset of 400 '\n",
            "            'million (image,\\n'\n",
            "            'text) pairs collected from the internet. After pre-training, '\n",
            "            'natural language\\n'\n",
            "            'is used to reference learned visual concepts (or describe new '\n",
            "            'ones) enabling\\n'\n",
            "            'zero-shot transfer of the model to downstream tasks. We study the '\n",
            "            'performance\\n'\n",
            "            'of this approach by benchmarking on over 30 different existing '\n",
            "            'computer vision\\n'\n",
            "            'datasets, spanning tasks such as OCR, action recognition in '\n",
            "            'videos,\\n'\n",
            "            'geo-localization, and many types of fine-grained object '\n",
            "            'classification. The\\n'\n",
            "            'model transfers non-trivially to most tasks and is often '\n",
            "            'competitive with a\\n'\n",
            "            'fully supervised baseline without the need for any dataset '\n",
            "            'specific training.\\n'\n",
            "            'For instance, we match the accuracy of the original ResNet-50 on '\n",
            "            'ImageNet\\n'\n",
            "            'zero-shot without needing to use any of the 1.28 million training '\n",
            "            'examples it\\n'\n",
            "            'was trained on. We release our code and pre-trained model weights '\n",
            "            'at\\n'\n",
            "            'https://github.com/OpenAI/CLIP.',\n",
            " 'Title': 'Learning Transferable Visual Models From Natural Language '\n",
            "          'Supervision'}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "pprint(doc_string)\n",
        "for i, chunks in enumerate(docs_chunks):\n",
        "    print(f\"Document {i}\")\n",
        "    print(f\" - # Chunks: {len(chunks)}\")\n",
        "    print(f\" - Metadata: \")\n",
        "    pprint(chunks[0].metadata)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHHbm1gGOiZq"
      },
      "source": [
        "## 3. Construct Your Document Vector Stores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icJCEaUKREAI",
        "outputId": "fcf379cc-3dc8-4118-9597-b02ec000bda6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Constructing Vector Stores\n",
            "CPU times: user 1.31 s, sys: 162 ms, total: 1.47 s\n",
            "Wall time: 38.9 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "print(\"Constructing Vector Stores\")\n",
        "vecstores = [FAISS.from_texts(extra_chunks, embedder)]\n",
        "vecstores += [FAISS.from_documents(doc_chunks, embedder) for doc_chunks in docs_chunks]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81JehDw7RD8t"
      },
      "outputs": [],
      "source": [
        "from faiss import IndexFlatL2\n",
        "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
        "\n",
        "embed_dims = len(embedder.embed_query(\"test\"))\n",
        "def default_FAISS():\n",
        "    '''Useful utility for making an empty FAISS vectorstore'''\n",
        "    return FAISS(\n",
        "        embedding_function=embedder,\n",
        "        index=IndexFlatL2(embed_dims),\n",
        "        docstore=InMemoryDocstore(),\n",
        "        index_to_docstore_id={},\n",
        "        normalize_L2=False\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pofnUFwGRD6C",
        "outputId": "6fe9a127-6a34-44ae-a16e-53579cd4d0ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Constructed aggregate docstore with 571 chunks\n"
          ]
        }
      ],
      "source": [
        "def aggregate_vstores(vectorstores):\n",
        "    ## Initialize an empty FAISS Index and merge others into it\n",
        "    agg_vstore = default_FAISS()\n",
        "    for vstore in vectorstores:\n",
        "        agg_vstore.merge_from(vstore)\n",
        "    return agg_vstore\n",
        "\n",
        "## Unintuitive optimization; merge_from seems to optimize constituent vector stores away\n",
        "docstore = aggregate_vstores(vecstores)\n",
        "\n",
        "print(f\"Constructed aggregate docstore with {len(docstore.docstore._dict)} chunks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJuXtQh6RD3s"
      },
      "source": [
        "## 4. Implement Your RAG Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MffpfJASRakW"
      },
      "outputs": [],
      "source": [
        "from langchain.document_transformers import LongContextReorder\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain_core.runnables.passthrough import RunnableAssign\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "import gradio as gr\n",
        "from functools import partial\n",
        "from operator import itemgetter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSYsUtB_SGLy"
      },
      "outputs": [],
      "source": [
        "## Utility Runnables/Methods\n",
        "def RPrint(preface=\"\"):\n",
        "    \"\"\"Simple passthrough \"prints, then returns\" chain\"\"\"\n",
        "    def print_and_return(x, preface):\n",
        "        if preface: print(preface, end=\"\")\n",
        "        pprint(x)\n",
        "        return x\n",
        "    return RunnableLambda(partial(print_and_return, preface=preface))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFDSPKuURah4"
      },
      "outputs": [],
      "source": [
        "convstore = default_FAISS()\n",
        "\n",
        "def save_memory_and_get_output(d, vstore):\n",
        "    \"\"\"Accepts 'input'/'output' dictionary and saves to convstore\"\"\"\n",
        "    vstore.add_texts([\n",
        "        f\"User previously responded with {d.get('input')}\",\n",
        "        f\"Agent previously responded with {d.get('output')}\"\n",
        "    ])\n",
        "    return d.get('output')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mEDV2U3Rafo"
      },
      "outputs": [],
      "source": [
        "initial_msg = (\n",
        "    \"Hello! I am a document chat agent here to help the user!\"\n",
        "    f\" I have access to the following documents: {doc_string}\\n\\nHow can I help you?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Avd2hAwFRoLU"
      },
      "outputs": [],
      "source": [
        "chat_prompt = ChatPromptTemplate.from_messages([(\"system\",\n",
        "    \"You are a document chatbot. Help the user as they ask questions about documents.\"\n",
        "    \" User messaged just asked: {input}\\n\\n\"\n",
        "    \" From this, we have retrieved the following potentially-useful info: \"\n",
        "    \" Conversation History Retrieval:\\n{history}\\n\\n\"\n",
        "    \" Document Retrieval:\\n{context}\\n\\n\"\n",
        "    \" (Answer only from retrieval. Only cite sources that are used. Make your response conversational.)\"\n",
        "), ('user', '{input}')])\n",
        "\n",
        "stream_chain = chat_prompt| RPrint() | instruct_llm | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wu0MgpWpRp_5"
      },
      "outputs": [],
      "source": [
        "\n",
        "retrieval_chain = (\n",
        "    {'input' : (lambda x: x)}\n",
        "    | RunnableAssign({'history' : lambda d: None})\n",
        "    | RunnableAssign({'context' : lambda d: None})\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNNUTa1uSIst"
      },
      "outputs": [],
      "source": [
        "\n",
        "def chat_gen(message, history=[], return_buffer=True):\n",
        "    buffer = \"\"\n",
        "\n",
        "    retrieval = retrieval_chain.invoke(message)\n",
        "    line_buffer = \"\"\n",
        "\n",
        "    ## Then, stream the results of the stream_chain\n",
        "    for token in stream_chain.stream(retrieval):\n",
        "        buffer += token\n",
        "        yield buffer if return_buffer else token\n",
        "\n",
        "    save_memory_and_get_output({'input':  message, 'output': buffer}, convstore)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6HootDHSSvP",
        "outputId": "053891fe-2ac3-43bd-dbc9-77961a85aa95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatPromptValue(messages=[SystemMessage(content='You are a document chatbot. Help the user as they ask questions about documents. User messaged just asked: Tell me about Attention mechanism!\\n\\n From this, we have retrieved the following potentially-useful info:  Conversation History Retrieval:\\nNone\\n\\n Document Retrieval:\\nNone\\n\\n (Answer only from retrieval. Only cite sources that are used. Make your response conversational.)', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me about Attention mechanism!', additional_kwargs={}, response_metadata={})])\n",
            "Sure, I'd be happy to explain! Attention mechanism is a concept in machine learning that is particularly useful in tasks like neural machine translation and image captioning, among others. It's a way for models to 'pay attention' to different parts of input data when producing outputs.\n",
            "\n",
            "In simpler terms, imagine you're translating a long sentence from English to French. Without attention, your model would have to consider the entire sentence as one big entity. However, with attention, the model can focus on specific parts of the sentence at different times, making it easier to understand the context and provide a more accurate translation.\n",
            "\n",
            "The concept of attention is inspired by human visual attention. When humans look at an image, we don't process the entire scene at once but sequentially focus on different parts of the image, and similarly, in language, we don't pay attention to every word at once but pay more attention to words that hold more contextual importance.\n",
            "\n",
            "Attention mechanisms were introduced in the context of neural networks by Bahdanau, et al., and have been used in various models like Transformer and BERT, which have led to significant improvements in machine learning tasks involving sequences of data.\n",
            "\n",
            "Source:\n",
            "Bahdanau, D., Cho, K., & Bengio, Y. (2014). Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473."
          ]
        }
      ],
      "source": [
        "## Start of Agent Event Loop\n",
        "test_question = \"Tell me about Attention mechanism!\"  ## <- modify as desired\n",
        "\n",
        "## Before you launch your gradio interface, make sure your thing works\n",
        "for response in chat_gen(test_question, return_buffer=False):\n",
        "    print(response, end='')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yph5SWwLSVZI"
      },
      "source": [
        "## 5. Interact with Gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 813
        },
        "id": "o1Jk23-CSnSU",
        "outputId": "fe0d2e1d-27ad-4982-9fc1-07ed03fcaa0b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-33-c5510b17b697>:1: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(value = [[None, initial_msg]])\n",
            "/usr/local/lib/python3.11/dist-packages/gradio/chat_interface.py:317: UserWarning: The gr.ChatInterface was not provided with a type, so the type of the gr.Chatbot, 'tuples', will be used.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://aaad8ea025825c1fd1.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://aaad8ea025825c1fd1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatPromptValue(messages=[SystemMessage(content='You are a document chatbot. Help the user as they ask questions about documents. User messaged just asked: what are Rag intensive tasks for nlp\\n\\n From this, we have retrieved the following potentially-useful info:  Conversation History Retrieval:\\nNone\\n\\n Document Retrieval:\\nNone\\n\\n (Answer only from retrieval. Only cite sources that are used. Make your response conversational.)', additional_kwargs={}, response_metadata={}), HumanMessage(content='what are Rag intensive tasks for nlp', additional_kwargs={}, response_metadata={})])\n",
            "ChatPromptValue(messages=[SystemMessage(content='You are a document chatbot. Help the user as they ask questions about documents. User messaged just asked: how many parameters where used in bert pretraining\\n\\n From this, we have retrieved the following potentially-useful info:  Conversation History Retrieval:\\nNone\\n\\n Document Retrieval:\\nNone\\n\\n (Answer only from retrieval. Only cite sources that are used. Make your response conversational.)', additional_kwargs={}, response_metadata={}), HumanMessage(content='how many parameters where used in bert pretraining', additional_kwargs={}, response_metadata={})])\n",
            "ChatPromptValue(messages=[SystemMessage(content='You are a document chatbot. Help the user as they ask questions about documents. User messaged just asked: only print the number of parameters and nothing else\\n\\n From this, we have retrieved the following potentially-useful info:  Conversation History Retrieval:\\nNone\\n\\n Document Retrieval:\\nNone\\n\\n (Answer only from retrieval. Only cite sources that are used. Make your response conversational.)', additional_kwargs={}, response_metadata={}), HumanMessage(content='only print the number of parameters and nothing else', additional_kwargs={}, response_metadata={})])\n",
            "ChatPromptValue(messages=[SystemMessage(content='You are a document chatbot. Help the user as they ask questions about documents. User messaged just asked: thank you\\n\\n From this, we have retrieved the following potentially-useful info:  Conversation History Retrieval:\\nNone\\n\\n Document Retrieval:\\nNone\\n\\n (Answer only from retrieval. Only cite sources that are used. Make your response conversational.)', additional_kwargs={}, response_metadata={}), HumanMessage(content='thank you', additional_kwargs={}, response_metadata={})])\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://aaad8ea025825c1fd1.gradio.live\n",
            "Closing server running on port: 7860\n"
          ]
        }
      ],
      "source": [
        "chatbot = gr.Chatbot(value = [[None, initial_msg]])\n",
        "demo = gr.ChatInterface(chat_gen, chatbot=chatbot).queue()\n",
        "\n",
        "try:\n",
        "     demo.launch(debug=True, share=True, show_api=False)\n",
        "     demo.close()\n",
        "except Exception as e:\n",
        "     demo.close()\n",
        "     print(e)\n",
        "     raise e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QB8um-sBSnQA",
        "outputId": "9b9e2900-e147-45de-ba6c-42a08a4a8d92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "docstore_index/\n",
            "docstore_index/index.pkl\n",
            "docstore_index/index.faiss\n"
          ]
        }
      ],
      "source": [
        "## Save and compress your index\n",
        "docstore.save_local(\"docstore_index\")\n",
        "!tar czvf docstore_index.tgz docstore_index\n",
        "\n",
        "!rm -rf docstore_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_lo00aISnOF",
        "outputId": "a4c53af0-6d20-4ad7-d32e-b7ce368339bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "docstore_index/\n",
            "docstore_index/index.pkl\n",
            "docstore_index/index.faiss\n",
            ". To demonstrate, we build an index using the DrQA [5]\\nWikipedia dump from December 2016 and compare outputs from RAG using this index to the newer\\nindex from our main results (December 2018). We prepare a list of 82 world leaders who had changed\\n7\\nTable 4: Human assessments for the Jeopardy\\nQuestion Generation Task.\\nFactuality\\nSpeci\\ufb01city\\nBART better\\n7.1%\\n16.8%\\nRAG better\\n42.7%\\n37.4%\\nBoth good\\n11.7%\\n11.8%\\nBoth poor\\n17.7%\\n6.9%\\nNo majority\\n20.8%\\n20.1%\\nTable 5: Ratio of distinct to total tri-grams for\\ngeneration tasks.\\nMSMARCO\\nJeopardy QGen\\nGold\\n89.6%\\n90.0%\\nBART\\n70.7%\\n32.4%\\nRAG-Token\\n77.8%\\n46.8%\\nRAG-Seq.\\n83.5%\\n53.8%\\nTable 6: Ablations on the dev set. As FEVER is a classi\\ufb01cation task, both RAG models are equivalent.\\nModel\\nNQ\\nTQA\\nWQ\\nCT\\nJeopardy-QGen\\nMSMarco\\nFVR-3\\nFVR-2\\nExact Match\\nB-1\\nQB-1\\nR-L\\nB-1\\nLabel Accuracy\\nRAG-Token-BM25\\n29.7\\n41.5\\n32.1\\n33.1\\n17.5\\n22.3\\n55.5\\n48.4\\n75.1\\n91.6\\nRAG-Sequence-BM25\\n31.8\\n44.1\\n36.6\\n33\n"
          ]
        }
      ],
      "source": [
        "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
        "!tar xzvf docstore_index.tgz\n",
        "new_db = FAISS.load_local(\"docstore_index\", embedder, allow_dangerous_deserialization=True)\n",
        "docs = new_db.similarity_search(\"Testing the index\")\n",
        "print(docs[0].page_content[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6jufYAwV0-k"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
