{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# RAG Evaluation\n",
        "## 1. Environment setup"
      ],
      "metadata": {
        "id": "oAkkVrT_XaS-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZ_jNe8rWbZz",
        "outputId": "de86740b-73ce-453b-87de-a036d435bfc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m187.2/187.2 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m487.4/487.4 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m418.7/418.7 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m606.1/606.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.11/dist-packages (0.3.20)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.48)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.21 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.21)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.38)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.13)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.0.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.8.1)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.11)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.0)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.21->langchain_community) (0.3.7)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.21->langchain_community) (2.10.6)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain_community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain_community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.21->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.21->langchain_community) (2.27.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "%pip install -q langchain langchain-nvidia-ai-endpoints gradio rich\n",
        "%pip install -q arxiv pymupdf faiss-cpu ragas\n",
        "%pip install langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"NVIDIA_API_KEY\"] = \"NVIDIA_API_KE\""
      ],
      "metadata": {
        "id": "xdX_ES2bWlme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "from rich.console import Console\n",
        "from rich.style import Style\n",
        "from rich.theme import Theme\n",
        "\n",
        "console = Console()\n",
        "base_style = Style(color=\"#76B900\", bold=True)\n",
        "norm_style = Style(bold=True)\n",
        "pprint = partial(console.print, style=base_style)\n",
        "pprint2 = partial(console.print, style=norm_style)\n",
        "\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
        "\n",
        "# NVIDIAEmbeddings.get_available_models()\n",
        "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
        "\n",
        "# ChatNVIDIA.get_available_models(base_url=\"http://llm_client:9000/v1\")\n",
        "instruct_llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\")"
      ],
      "metadata": {
        "id": "GXzWf2wcWuaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. PairWise Evaluater\n",
        "### 1.  Pull In Your Document Retrieval Index"
      ],
      "metadata": {
        "id": "6llEo3W_Wxjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "!tar xzvf docstore_index.tgz\n",
        "docstore = FAISS.load_local(\"docstore_index\", embedder, allow_dangerous_deserialization=True)\n",
        "docs = list(docstore.docstore._dict.values())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9T84NoGXryN",
        "outputId": "9335add0-42db-43c8-eef8-d97165fe28fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "docstore_index/\n",
            "docstore_index/index.pkl\n",
            "docstore_index/index.faiss\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def format_chunk(doc):\n",
        "    return (\n",
        "        f\"Paper: {doc.metadata.get('Title', 'unknown')}\"\n",
        "        f\"\\n\\nSummary: {doc.metadata.get('Summary', 'unknown')}\"\n",
        "        f\"\\n\\nPage Body: {doc.page_content}\"\n",
        "    )\n",
        "\n",
        "## This printout just confirms that your store has been retrieved\n",
        "pprint(f\"Constructed aggregate docstore with {len(docstore.docstore._dict)} chunks\")\n",
        "pprint(f\"Sample Chunk:\")\n",
        "print(format_chunk(docs[len(docs)//2]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 757
        },
        "id": "iNe5oGQwYGVn",
        "outputId": "85b1dfcc-abc7-4d0a-960a-86e1076b1e47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0mConstructed aggregate docstore with \u001b[0m\u001b[1;36m571\u001b[0m\u001b[1;38;2;118;185;0m chunks\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Constructed aggregate docstore with </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">571</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> chunks</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0mSample Chunk:\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Sample Chunk:</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paper: ReAct: Synergizing Reasoning and Acting in Language Models\n",
            "\n",
            "Summary: While large language models (LLMs) have demonstrated impressive capabilities\n",
            "across tasks in language understanding and interactive decision making, their\n",
            "abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.\n",
            "action plan generation) have primarily been studied as separate topics. In this\n",
            "paper, we explore the use of LLMs to generate both reasoning traces and\n",
            "task-specific actions in an interleaved manner, allowing for greater synergy\n",
            "between the two: reasoning traces help the model induce, track, and update\n",
            "action plans as well as handle exceptions, while actions allow it to interface\n",
            "with external sources, such as knowledge bases or environments, to gather\n",
            "additional information. We apply our approach, named ReAct, to a diverse set of\n",
            "language and decision making tasks and demonstrate its effectiveness over\n",
            "state-of-the-art baselines, as well as improved human interpretability and\n",
            "trustworthiness over methods without reasoning or acting components.\n",
            "Concretely, on question answering (HotpotQA) and fact verification (Fever),\n",
            "ReAct overcomes issues of hallucination and error propagation prevalent in\n",
            "chain-of-thought reasoning by interacting with a simple Wikipedia API, and\n",
            "generates human-like task-solving trajectories that are more interpretable than\n",
            "baselines without reasoning traces. On two interactive decision making\n",
            "benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and\n",
            "reinforcement learning methods by an absolute success rate of 34% and 10%\n",
            "respectively, while being prompted with only one or two in-context examples.\n",
            "Project site with code: https://react-lm.github.io\n",
            "\n",
            "Page Body: LLMs, language as a fundamental cognitive mechanism will play a critical role in interaction and\n",
            "decision making. What is more, progress in LLMs has also inspired the development of versatile and\n",
            "generalist agents like Reed et al. (2022).\n",
            "6\n",
            "CONCLUSION\n",
            "We have proposed ReAct – a simple yet effective method for synergizing reasoning and acting in\n",
            "large language models. Through a diverse set of experiments on multi-hop question-answering, fact\n",
            "checking, and interactive decision-making tasks, we show that ReAct leads to superior performance\n",
            "with interpretable decision traces. Despite the simplicity of our method, complex tasks with large\n",
            "action spaces require more demonstrations to learn well, which unfortunately can easily go beyond\n",
            "the input length limit of in-context learning. We explore the ﬁne-tuning approach on HotpotQA\n",
            "6Human feedback can also be incorporated in a complementary manner but we leave it for future work.\n",
            "9\n",
            "Published as a conference paper at ICLR 2023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Pull In Your RAG Chain"
      ],
      "metadata": {
        "id": "KOKOrbPlYQwk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableLambda, RunnableBranch\n",
        "from langchain_core.runnables.passthrough import RunnableAssign\n",
        "from langchain.document_transformers import LongContextReorder\n",
        "\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
        "\n",
        "from functools import partial\n",
        "from operator import itemgetter\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "# NVIDIAEmbeddings.get_available_models()\n",
        "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
        "\n",
        "# ChatNVIDIA.get_available_models()\n",
        "instruct_llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\")\n",
        "llm = instruct_llm | StrOutputParser()\n",
        "\n",
        "def docs2str(docs, title=\"Document\"):\n",
        "    \"\"\"Useful utility for making chunks into context string. Optional, but useful\"\"\"\n",
        "    out_str = \"\"\n",
        "    for doc in docs:\n",
        "        doc_name = getattr(doc, 'metadata', {}).get('Title', title)\n",
        "        if doc_name: out_str += f\"[Quote from {doc_name}] \"\n",
        "        out_str += getattr(doc, 'page_content', str(doc)) + \"\\n\"\n",
        "    return out_str\n"
      ],
      "metadata": {
        "id": "CmFs3N1CYY9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_prompt = ChatPromptTemplate.from_template(\n",
        "    \"You are a document chatbot. Help the user as they ask questions about documents.\"\n",
        "    \" User messaged just asked you a question: {input}\\n\\n\"\n",
        "    \" The following information may be useful for your response: \"\n",
        "    \" Document Retrieval:\\n{context}\\n\\n\"\n",
        "    \" (Answer only from retrieval. Only cite sources that are used. Make your response conversational)\"\n",
        "    \"\\n\\nUser Question: {input}\"\n",
        ")"
      ],
      "metadata": {
        "id": "_suht0_JYY7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def output_puller(inputs):\n",
        "    \"\"\"\"Output generator. Useful if your chain returns a dictionary with key 'output'\"\"\"\n",
        "    if isinstance(inputs, dict):\n",
        "        inputs = [inputs]\n",
        "    for token in inputs:\n",
        "        if token.get('output'):\n",
        "            yield token.get('output')\n"
      ],
      "metadata": {
        "id": "C7a_fmMJYwtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "## Chain 1 Specs: \"Hello World\" -> retrieval_chain\n",
        "##   -> {'input': <str>, 'context' : <str>}\n",
        "long_reorder = RunnableLambda(LongContextReorder().transform_documents)  ## GIVEN\n",
        "context_getter = RunnableLambda(lambda x: x)\n",
        "retrieval_chain = {'input' : (lambda x: x)} | RunnableAssign({'context' : context_getter})\n",
        "\n",
        "##   -> {\"output\" : <str>, ...} -> output_puller\n",
        "generator_chain = RunnableLambda(lambda x: x)  ## TODO\n",
        "generator_chain = {'output' : generator_chain} | RunnableLambda(output_puller)  ## GIVEN\n"
      ],
      "metadata": {
        "id": "0Jvg7XAeYyKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "rag_chain = retrieval_chain | generator_chain\n",
        "\n",
        "# pprint(rag_chain.invoke(\"Tell me something interesting!\"))\n",
        "for token in rag_chain.stream(\"Tell me something interesting!\"):\n",
        "    print(token, end=\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXUsmgTFY04C",
        "outputId": "2d27e29f-722b-4a82-927b-e4590158aff4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input': 'Tell me something interesting!', 'context': {'input': 'Tell me something interesting!'}}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Generating Synthetic Question-Answer Pairs"
      ],
      "metadata": {
        "id": "SkCqtzVqY3Bw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "num_questions = 3\n",
        "synth_questions = []\n",
        "synth_answers = []\n",
        "\n",
        "simple_prompt = ChatPromptTemplate.from_messages([('system', '{system}'), ('user', 'INPUT: {input}')])\n"
      ],
      "metadata": {
        "id": "hX529_TdY7gZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(num_questions):\n",
        "    doc1, doc2 = random.sample(docs, 2)\n",
        "    sys_msg = (\n",
        "        \"Use the documents provided by the user to generate an interesting question-answer pair.\"\n",
        "        \" Try to use both documents if possible, and rely more on the document bodies than the summary.\"\n",
        "        \" Use the format:\\nQuestion: (good question, 1-3 sentences, detailed)\\n\\nAnswer: (answer derived from the documents)\"\n",
        "        \" DO NOT SAY: \\\"Here is an interesting question pair\\\" or similar. FOLLOW FORMAT!\"\n",
        "    )\n",
        "    usr_msg = (\n",
        "        f\"Document1: {format_chunk(doc1)}\\n\\n\"\n",
        "        f\"Document2: {format_chunk(doc2)}\"\n",
        "    )\n",
        "\n",
        "    qa_pair = (simple_prompt | llm).invoke({'system': sys_msg, 'input': usr_msg})\n",
        "    synth_questions += [qa_pair.split('\\n\\n')[0]]\n",
        "    synth_answers += [qa_pair.split('\\n\\n')[1]]\n",
        "    pprint2(f\"QA Pair {i+1}\")\n",
        "    pprint2(synth_questions[-1])\n",
        "    pprint(synth_answers[-1])\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "d1EWNYyfY7dv",
        "outputId": "0a17b354-b737-45a0-c8cf-6c66bc876294"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mQA Pair \u001b[0m\u001b[1;36m1\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mQuestion: How does the ReAct approach address the issues of hallucination and error propagation prevalent in \u001b[0m\n",
              "\u001b[1mchain-of-thought reasoning, and what improvement does it bring to the task of question answering?\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Question: How does the ReAct approach address the issues of hallucination and error propagation prevalent in </span>\n",
              "<span style=\"font-weight: bold\">chain-of-thought reasoning, and what improvement does it bring to the task of question answering?</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0mAnswer: According to the paper, ReAct overcomes issues of hallucination and error propagation prevalent in \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mchain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mtrajectories that are more interpretable than baselines without reasoning traces. Specifically, on question \u001b[0m\n",
              "\u001b[1;38;2;118;185;0manswering tasks like HotpotQA, ReAct is able to retrieve up-to-date information from the Internet and provide a \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mreasonable answer, whereas other methods like Standard and CoT give wrong answers due to hallucination.\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Answer: According to the paper, ReAct overcomes issues of hallucination and error propagation prevalent in </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">trajectories that are more interpretable than baselines without reasoning traces. Specifically, on question </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">answering tasks like HotpotQA, ReAct is able to retrieve up-to-date information from the Internet and provide a </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">reasonable answer, whereas other methods like Standard and CoT give wrong answers due to hallucination.</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mQA Pair \u001b[0m\u001b[1;36m2\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mQuestion: Can a model trained on natural language supervision from a massive dataset of image-text pairs learn to \u001b[0m\n",
              "\u001b[1mreason and access various types of knowledge beyond visual recognition, such as mathematical information or current\u001b[0m\n",
              "\u001b[1mevents?\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Question: Can a model trained on natural language supervision from a massive dataset of image-text pairs learn to </span>\n",
              "<span style=\"font-weight: bold\">reason and access various types of knowledge beyond visual recognition, such as mathematical information or current</span>\n",
              "<span style=\"font-weight: bold\">events?</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0mAnswer: The MRKL architecture discussed in Document2 suggests that a model can potentially overcome the limitations\u001b[0m\n",
              "\u001b[1;38;2;118;185;0mof large language models \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mLMs\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m by incorporating discrete knowledge and reasoning modules. However, the performance \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mof the model described in Document1, CLIP, is limited to zero-shot transfer and relies heavily on visual \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mrecognition, with poor performance on fine-grained detection and no evidence of accessing knowledge beyond the \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mtraining data. Nevertheless, the existence of the MRKL architecture and its implementation in Jurassic-X hint at \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mthe possibility of future advancements in language models, enabling them to access and reason about various types \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mof knowledge.\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Answer: The MRKL architecture discussed in Document2 suggests that a model can potentially overcome the limitations</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">of large language models (LMs) by incorporating discrete knowledge and reasoning modules. However, the performance </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">of the model described in Document1, CLIP, is limited to zero-shot transfer and relies heavily on visual </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">recognition, with poor performance on fine-grained detection and no evidence of accessing knowledge beyond the </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">training data. Nevertheless, the existence of the MRKL architecture and its implementation in Jurassic-X hint at </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">the possibility of future advancements in language models, enabling them to access and reason about various types </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">of knowledge.</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mQA Pair \u001b[0m\u001b[1;36m3\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mQuestion: How can large language models \u001b[0m\u001b[1m(\u001b[0m\u001b[1mLLMs\u001b[0m\u001b[1m)\u001b[0m\u001b[1m be utilized to improve their reasoning and acting abilities, \u001b[0m\n",
              "\u001b[1mespecially in handling exceptions and interacting with external sources?\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Question: How can large language models (LLMs) be utilized to improve their reasoning and acting abilities, </span>\n",
              "<span style=\"font-weight: bold\">especially in handling exceptions and interacting with external sources?</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0mAnswer: According to the paper \u001b[0m\u001b[32m\"ReAct: Synergizing Reasoning and Acting in Language Models\"\u001b[0m\u001b[1;38;2;118;185;0m, the approach, named \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mReAct, involves generating both reasoning traces and task-specific actions in an interleaved manner. This allows \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mthe model to induce, track, and update action plans, handle exceptions, and interface with external sources such as\u001b[0m\n",
              "\u001b[1;38;2;118;185;0mknowledge bases or environments to gather additional information.\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Answer: According to the paper </span><span style=\"color: #008000; text-decoration-color: #008000\">\"ReAct: Synergizing Reasoning and Acting in Language Models\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, the approach, named </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">ReAct, involves generating both reasoning traces and task-specific actions in an interleaved manner. This allows </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">the model to induce, track, and update action plans, handle exceptions, and interface with external sources such as</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">knowledge bases or environments to gather additional information.</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.  Answer The Synthetic Questions"
      ],
      "metadata": {
        "id": "eCII9vngY7bt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "rag_answers = []\n",
        "for i, q in enumerate(synth_questions):\n",
        "    ## TODO: Compute the RAG Answer\n",
        "    rag_answer = \"\"\n",
        "    rag_answers += [rag_answer]\n",
        "    pprint2(f\"QA Pair {i+1}\", q, \"\", sep=\"\\n\")\n",
        "    pprint(f\"RAG Answer: {rag_answer}\", \"\", sep='\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "Wi9M4UgoZK-_",
        "outputId": "4d9d0cab-cacb-4ac2-c6c0-ec722a5918f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mQA Pair \u001b[0m\u001b[1;36m1\u001b[0m\n",
              "\u001b[1mQuestion: How does the ReAct approach address the issues of hallucination and error propagation prevalent in \u001b[0m\n",
              "\u001b[1mchain-of-thought reasoning, and what improvement does it bring to the task of question answering?\u001b[0m\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
              "<span style=\"font-weight: bold\">Question: How does the ReAct approach address the issues of hallucination and error propagation prevalent in </span>\n",
              "<span style=\"font-weight: bold\">chain-of-thought reasoning, and what improvement does it bring to the task of question answering?</span>\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0mRAG Answer: \u001b[0m\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: </span>\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mQA Pair \u001b[0m\u001b[1;36m2\u001b[0m\n",
              "\u001b[1mQuestion: Can a model trained on natural language supervision from a massive dataset of image-text pairs learn to \u001b[0m\n",
              "\u001b[1mreason and access various types of knowledge beyond visual recognition, such as mathematical information or current\u001b[0m\n",
              "\u001b[1mevents?\u001b[0m\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
              "<span style=\"font-weight: bold\">Question: Can a model trained on natural language supervision from a massive dataset of image-text pairs learn to </span>\n",
              "<span style=\"font-weight: bold\">reason and access various types of knowledge beyond visual recognition, such as mathematical information or current</span>\n",
              "<span style=\"font-weight: bold\">events?</span>\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0mRAG Answer: \u001b[0m\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: </span>\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mQA Pair \u001b[0m\u001b[1;36m3\u001b[0m\n",
              "\u001b[1mQuestion: How can large language models \u001b[0m\u001b[1m(\u001b[0m\u001b[1mLLMs\u001b[0m\u001b[1m)\u001b[0m\u001b[1m be utilized to improve their reasoning and acting abilities, \u001b[0m\n",
              "\u001b[1mespecially in handling exceptions and interacting with external sources?\u001b[0m\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
              "<span style=\"font-weight: bold\">Question: How can large language models (LLMs) be utilized to improve their reasoning and acting abilities, </span>\n",
              "<span style=\"font-weight: bold\">especially in handling exceptions and interacting with external sources?</span>\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0mRAG Answer: \u001b[0m\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: </span>\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.  Implement A Human Preference Metric"
      ],
      "metadata": {
        "id": "tKeuhl96ZK8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## If it's llama, maybe system message would be good?\n",
        "eval_prompt = ChatPromptTemplate.from_template(\"\"\"INSTRUCTION:\n",
        "Evaluate the following Question-Answer pair for human preference and consistency.\n",
        "Assume the first answer is a ground truth answer and has to be correct.\n",
        "Assume the second answer may or may not be true.\n",
        "[1] The second answer lies, does not answer the question, or is inferior to the first answer.\n",
        "[2] The second answer is better than the first and does not introduce any inconsistencies.\n",
        "\n",
        "Output Format:\n",
        "[Score] Justification\n",
        "\n",
        "{qa_trio}\n",
        "\n",
        "EVALUATION:\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "id": "n627vA-kZK6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pref_score = []\n",
        "\n",
        "trio_gen = zip(synth_questions, synth_answers, rag_answers)\n",
        "for i, (q, a_synth, a_rag) in enumerate(trio_gen):\n",
        "    pprint2(f\"Set {i+1}\\n\\nQuestion: {q}\\n\\n\")\n",
        "\n",
        "    qa_trio = f\"Question: {q}\\n\\nAnswer 1 (Ground Truth): {a_synth}\\n\\n Answer 2 (New Answer): {a_rag}\"\n",
        "    pref_score += [(eval_prompt | llm).invoke({'qa_trio': qa_trio})]\n",
        "    pprint(f\"Synth Answer: {a_synth}\\n\\n\")\n",
        "    pprint(f\"RAG Answer: {a_rag}\\n\\n\")\n",
        "    pprint2(f\"Synth Evaluation: {pref_score[-1]}\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3ANNESLDZSgi",
        "outputId": "e0b766f1-8cec-4cfc-999f-fc3cce49cf0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mSet \u001b[0m\u001b[1;36m1\u001b[0m\n",
              "\n",
              "\u001b[1mQuestion: Question: How does the ReAct approach address the issues of hallucination and error propagation prevalent\u001b[0m\n",
              "\u001b[1min chain-of-thought reasoning, and what improvement does it bring to the task of question answering?\u001b[0m\n",
              "\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Set </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
              "\n",
              "<span style=\"font-weight: bold\">Question: Question: How does the ReAct approach address the issues of hallucination and error propagation prevalent</span>\n",
              "<span style=\"font-weight: bold\">in chain-of-thought reasoning, and what improvement does it bring to the task of question answering?</span>\n",
              "\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0mSynth Answer: Answer: According to the paper, ReAct overcomes issues of hallucination and error propagation \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mprevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mtask-solving trajectories that are more interpretable than baselines without reasoning traces. Specifically, on \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mquestion answering tasks like HotpotQA, ReAct is able to retrieve up-to-date information from the Internet and \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mprovide a reasonable answer, whereas other methods like Standard and CoT give wrong answers due to hallucination.\u001b[0m\n",
              "\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Synth Answer: Answer: According to the paper, ReAct overcomes issues of hallucination and error propagation </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">task-solving trajectories that are more interpretable than baselines without reasoning traces. Specifically, on </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">question answering tasks like HotpotQA, ReAct is able to retrieve up-to-date information from the Internet and </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">provide a reasonable answer, whereas other methods like Standard and CoT give wrong answers due to hallucination.</span>\n",
              "\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0mRAG Answer: \u001b[0m\n",
              "\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: </span>\n",
              "\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mSynth Evaluation: There is no Answer \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m provided. Please provide Answer \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m, and I will evaluate the question-answer \u001b[0m\n",
              "\u001b[1mpair for human preference and consistency based on the instructions.\u001b[0m\n",
              "\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Synth Evaluation: There is no Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\"> provided. Please provide Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">, and I will evaluate the question-answer </span>\n",
              "<span style=\"font-weight: bold\">pair for human preference and consistency based on the instructions.</span>\n",
              "\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mSet \u001b[0m\u001b[1;36m2\u001b[0m\n",
              "\n",
              "\u001b[1mQuestion: Question: Can a model trained on natural language supervision from a massive dataset of image-text pairs \u001b[0m\n",
              "\u001b[1mlearn to reason and access various types of knowledge beyond visual recognition, such as mathematical information \u001b[0m\n",
              "\u001b[1mor current events?\u001b[0m\n",
              "\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Set </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
              "\n",
              "<span style=\"font-weight: bold\">Question: Question: Can a model trained on natural language supervision from a massive dataset of image-text pairs </span>\n",
              "<span style=\"font-weight: bold\">learn to reason and access various types of knowledge beyond visual recognition, such as mathematical information </span>\n",
              "<span style=\"font-weight: bold\">or current events?</span>\n",
              "\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0mSynth Answer: Answer: The MRKL architecture discussed in Document2 suggests that a model can potentially overcome \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mthe limitations of large language models \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mLMs\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m by incorporating discrete knowledge and reasoning modules. However, \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mthe performance of the model described in Document1, CLIP, is limited to zero-shot transfer and relies heavily on \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mvisual recognition, with poor performance on fine-grained detection and no evidence of accessing knowledge beyond \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mthe training data. Nevertheless, the existence of the MRKL architecture and its implementation in Jurassic-X hint \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mat the possibility of future advancements in language models, enabling them to access and reason about various \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mtypes of knowledge.\u001b[0m\n",
              "\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Synth Answer: Answer: The MRKL architecture discussed in Document2 suggests that a model can potentially overcome </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">the limitations of large language models (LMs) by incorporating discrete knowledge and reasoning modules. However, </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">the performance of the model described in Document1, CLIP, is limited to zero-shot transfer and relies heavily on </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">visual recognition, with poor performance on fine-grained detection and no evidence of accessing knowledge beyond </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">the training data. Nevertheless, the existence of the MRKL architecture and its implementation in Jurassic-X hint </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">at the possibility of future advancements in language models, enabling them to access and reason about various </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">types of knowledge.</span>\n",
              "\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0mRAG Answer: \u001b[0m\n",
              "\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: </span>\n",
              "\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mSynth Evaluation: However, you didn't provide the second answer. Please provide Answer \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m, and I'll be happy to \u001b[0m\n",
              "\u001b[1mevaluate and provide a score along with a justification.\u001b[0m\n",
              "\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Synth Evaluation: However, you didn't provide the second answer. Please provide Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">, and I'll be happy to </span>\n",
              "<span style=\"font-weight: bold\">evaluate and provide a score along with a justification.</span>\n",
              "\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mSet \u001b[0m\u001b[1;36m3\u001b[0m\n",
              "\n",
              "\u001b[1mQuestion: Question: How can large language models \u001b[0m\u001b[1m(\u001b[0m\u001b[1mLLMs\u001b[0m\u001b[1m)\u001b[0m\u001b[1m be utilized to improve their reasoning and acting \u001b[0m\n",
              "\u001b[1mabilities, especially in handling exceptions and interacting with external sources?\u001b[0m\n",
              "\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Set </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
              "\n",
              "<span style=\"font-weight: bold\">Question: Question: How can large language models (LLMs) be utilized to improve their reasoning and acting </span>\n",
              "<span style=\"font-weight: bold\">abilities, especially in handling exceptions and interacting with external sources?</span>\n",
              "\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0mSynth Answer: Answer: According to the paper \u001b[0m\u001b[32m\"ReAct: Synergizing Reasoning and Acting in Language Models\"\u001b[0m\u001b[1;38;2;118;185;0m, the \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mapproach, named ReAct, involves generating both reasoning traces and task-specific actions in an interleaved \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mmanner. This allows the model to induce, track, and update action plans, handle exceptions, and interface with \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mexternal sources such as knowledge bases or environments to gather additional information.\u001b[0m\n",
              "\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Synth Answer: Answer: According to the paper </span><span style=\"color: #008000; text-decoration-color: #008000\">\"ReAct: Synergizing Reasoning and Acting in Language Models\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, the </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">approach, named ReAct, involves generating both reasoning traces and task-specific actions in an interleaved </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">manner. This allows the model to induce, track, and update action plans, handle exceptions, and interface with </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">external sources such as knowledge bases or environments to gather additional information.</span>\n",
              "\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0mRAG Answer: \u001b[0m\n",
              "\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: </span>\n",
              "\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mSynth Evaluation: To evaluate the Question-Answer pair, I will review Answer \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m against the criteria outlined.\u001b[0m\n",
              "\n",
              "\u001b[1mScore: \u001b[0m\n",
              "\n",
              "\u001b[1mJustification: \u001b[0m\n",
              "\u001b[1mAnswer \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m lies because it does not explain how large language models \u001b[0m\u001b[1m(\u001b[0m\u001b[1mLLMs\u001b[0m\u001b[1m)\u001b[0m\u001b[1m can utilize external sources to gather \u001b[0m\n",
              "\u001b[1minformation, which is an important aspect of handling exceptions and interacting with external sources.\u001b[0m\n",
              "\n",
              "\u001b[1mANSWER \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m: \u001b[0m\n",
              "\n",
              "\u001b[32m\"In contrast to ReAct, we propose a novel approach called EXCEL, which leverages external knowledge graphs to \u001b[0m\n",
              "\u001b[32mimprove the acting capabilities of LLMs. EXCEL achieves this by retrieving and incorporating relevant information \u001b[0m\n",
              "\u001b[32mfrom knowledge graphs, thereby enabling the model to handle exceptions and interface with external sources more \u001b[0m\n",
              "\u001b[32meffectively.\"\u001b[0m\n",
              "\n",
              "\u001b[1mThis answer introduces inconsistencies with Answer \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m by introducing a new approach, EXCEL, which is not mentioned \u001b[0m\n",
              "\u001b[1min the ground truth answer. It also corrects a limitation in the original approach by providing an alternative \u001b[0m\n",
              "\u001b[1msolution. \u001b[0m\n",
              "\n",
              "\u001b[1mHowever, considering the above-mentioned answers, I must correct my previous statement.\u001b[0m\n",
              "\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Synth Evaluation: To evaluate the Question-Answer pair, I will review Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\"> against the criteria outlined.</span>\n",
              "\n",
              "<span style=\"font-weight: bold\">Score: </span>\n",
              "\n",
              "<span style=\"font-weight: bold\">Justification: </span>\n",
              "<span style=\"font-weight: bold\">Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\"> lies because it does not explain how large language models (LLMs) can utilize external sources to gather </span>\n",
              "<span style=\"font-weight: bold\">information, which is an important aspect of handling exceptions and interacting with external sources.</span>\n",
              "\n",
              "<span style=\"font-weight: bold\">ANSWER </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">: </span>\n",
              "\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">\"In contrast to ReAct, we propose a novel approach called EXCEL, which leverages external knowledge graphs to </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">improve the acting capabilities of LLMs. EXCEL achieves this by retrieving and incorporating relevant information </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">from knowledge graphs, thereby enabling the model to handle exceptions and interface with external sources more </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">effectively.\"</span>\n",
              "\n",
              "<span style=\"font-weight: bold\">This answer introduces inconsistencies with Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\"> by introducing a new approach, EXCEL, which is not mentioned </span>\n",
              "<span style=\"font-weight: bold\">in the ground truth answer. It also corrects a limitation in the original approach by providing an alternative </span>\n",
              "<span style=\"font-weight: bold\">solution. </span>\n",
              "\n",
              "<span style=\"font-weight: bold\">However, considering the above-mentioned answers, I must correct my previous statement.</span>\n",
              "\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pref_score = sum((\"[2]\" in score) for score in pref_score) / len(pref_score)\n",
        "print(f\"Preference Score: {pref_score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnYL4W86ZSeg",
        "outputId": "39e7951a-744d-44f7-9ff9-7b581654adba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preference Score: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%js\n",
        "var url = 'http://'+window.location.host+':8090';\n",
        "element.innerHTML = '<a style=\"color:green;\" target=\"_blank\" href='+url+'><h1>< Link To Gradio Frontend ></h1></a>';"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "X9VNjDu7Zmu2",
        "outputId": "5443808f-50fe-45af-81b4-3c94069e3133"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "var url = 'http://'+window.location.host+':8090';\n",
              "element.innerHTML = '<a style=\"color:green;\" target=\"_blank\" href='+url+'><h1>< Link To Gradio Frontend ></h1></a>';\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-QPrqbK1Zyrf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}