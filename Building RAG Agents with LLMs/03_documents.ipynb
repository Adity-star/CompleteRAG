{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-045vtSr3r7"
      },
      "source": [
        "# 1. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdMWYWXyqoI8",
        "outputId": "a245857c-3033-41b1-c660-8ee3e0e64da4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.20-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.45)\n",
            "Collecting langchain<1.0.0,>=0.3.21 (from langchain-community)\n",
            "  Downloading langchain-0.3.21-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.39)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.14)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.0.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.15)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting langchain-text-splitters<1.0.0,>=0.3.7 (from langchain<1.0.0,>=0.3.21->langchain-community)\n",
            "  Downloading langchain_text_splitters-0.3.7-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.21->langchain-community) (2.10.6)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.21->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.21->langchain-community) (2.27.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
            "Downloading langchain_community-0.3.20-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain-0.3.21-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
            "Downloading langchain_text_splitters-0.3.7-py3-none-any.whl (32 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-text-splitters, langchain, langchain-community\n",
            "  Attempting uninstall: langchain-text-splitters\n",
            "    Found existing installation: langchain-text-splitters 0.3.6\n",
            "    Uninstalling langchain-text-splitters-0.3.6:\n",
            "      Successfully uninstalled langchain-text-splitters-0.3.6\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.20\n",
            "    Uninstalling langchain-0.3.20:\n",
            "      Successfully uninstalled langchain-0.3.20\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-0.3.21 langchain-community-0.3.20 langchain-text-splitters-0.3.7 marshmallow-3.26.1 mypy-extensions-1.0.0 pydantic-settings-2.8.1 python-dotenv-1.0.1 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "%pip install -qq langchain langchain-nvidia-ai-endpoints gradio\n",
        "%pip install -qq arxiv pymupdf\n",
        "%pip install -U langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2hUWLXsqrn3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"NVIDIA_API_KEY\"] = \"NVIDIA_API_KEY\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YH1n2Fn4q4_2"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "from rich.console import Console\n",
        "from rich.style import Style\n",
        "from rich.theme import Theme\n",
        "\n",
        "console = Console()\n",
        "base_style = Style(color=\"#76B900\", bold=True)\n",
        "pprint = partial(console.print, style=base_style)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZxX3rj9rP9w"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "from functools import partial\n",
        "\n",
        "def Rprint(perface='State: '):\n",
        "  def print_and_return(x, perface=''):\n",
        "    print(f'{perface}{x}')\n",
        "    return x\n",
        "  return RunnableLambda(partial(print_and_return, perface=perface))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNJfQ0PBrRqD"
      },
      "outputs": [],
      "source": [
        "def PPrint(preface=\"State: \"):\n",
        "    def print_and_return(x, preface=\"\"):\n",
        "        pprint(preface, x)\n",
        "        return x\n",
        "    return RunnableLambda(partial(print_and_return, preface=preface))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Miwnw8Ghr0ll"
      },
      "source": [
        "# 2. Loading Documents\n",
        "* `UnstructuredFileLoader`: Generally-useful file loader for arbitrary files; doesn't make too many assumptions about your document structure and is usually sufficient.\n",
        "* `ArxivLoader`: A more specialized file-loader which can communicate with the Arxiv interface directly. Just one example of many, this will make some more assumptions about your data to yield nicer parsings and auto-fill metadata (useful when you have multiple documents/formats)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNu9EMa-syec"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import UnstructuredFileLoader\n",
        "from langchain.document_loaders import ArxivLoader\n",
        "\n",
        "\n",
        "documents = ArxivLoader(query=\"2404.16130\").load() # its about GraphRAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMJ8FK2wtpGv",
        "outputId": "c50554f6-6751-4eeb-e6c8-5542ddfc6635"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Documents Retrieved: 1\n",
            "Sample of Document 1 Content (Total Length: 89583):\n",
            "From Local to Global: A GraphRAG Approach to\n",
            "Query-Focused Summarization\n",
            "Darren Edge1†\n",
            "Ha Trinh1†\n",
            "Newman Cheng2\n",
            "Joshua Bradley2\n",
            "Alex Chao3\n",
            "Apurva Mody3\n",
            "Steven Truitt2\n",
            "Dasha Metropolitansky1\n",
            "Robert Osazuwa Ness1\n",
            "Jonathan Larson1\n",
            "1Microsoft Research\n",
            "2Microsoft Strategic Missions and Technologies\n",
            "3Microsoft Office of the CTO\n",
            "{daedge,trinhha,newmancheng,joshbradley,achao,moapurva,\n",
            "steventruitt,dasham,robertness,jolarso}@microsoft.com\n",
            "†These authors contributed equally to this work\n",
            "Abstract\n",
            "The use of retrieval-augmented generation (RAG) to retrieve relevant informa-\n",
            "tion from an external knowledge source enables large language models (LLMs)\n",
            "to answer questions over private and/or previously unseen document collections.\n",
            "However, RAG fails on global questions directed at an entire text corpus, such\n",
            "as “What are the main themes in the dataset?”, since this is inherently a query-\n",
            "focused summarization (QFS) task, rather than an explicit retrieval task. Prior\n",
            "QFS methods, meanwhile, do not scal\n"
          ]
        }
      ],
      "source": [
        "## Printing out a sample of the content\n",
        "print(\"Number of Documents Retrieved:\", len(documents))\n",
        "print(f\"Sample of Document 1 Content (Total Length: {len(documents[0].page_content)}):\")\n",
        "print(documents[0].page_content[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "uKRxMzJdu056",
        "outputId": "75adc657-1f62-483a-d204-c0988486ccef"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2025-02-19'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'From Local to Global: A Graph RAG Approach to Query-Focused Summarization'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Dasha </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Metropolitansky, Robert Osazuwa Ness, Jonathan Larson'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'The use of retrieval-augmented generation (RAG) to retrieve relevant\\ninformation from an external </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">knowledge source enables large language models\\n(LLMs) to answer questions over private and/or previously unseen </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">document\\ncollections. However, RAG fails on global questions directed at an entire text\\ncorpus, such as \"What are</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">the main themes in the dataset?\", since this is\\ninherently a query-focused summarization (QFS) task, rather than </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">an explicit\\nretrieval task. Prior QFS methods, meanwhile, do not scale to the quantities of\\ntext indexed by </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">typical RAG systems. To combine the strengths of these\\ncontrasting methods, we propose GraphRAG, a graph-based </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">approach to question\\nanswering over private text corpora that scales with both the generality of\\nuser questions </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">and the quantity of source text. Our approach uses an LLM to\\nbuild a graph index in two stages: first, to derive </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">an entity knowledge graph\\nfrom the source documents, then to pregenerate community summaries for all\\ngroups of </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">closely related entities. Given a question, each community summary is\\nused to generate a partial response, before </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">all partial responses are again\\nsummarized in a final response to the user. For a class of global </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">sensemaking\\nquestions over datasets in the 1 million token range, we show that GraphRAG\\nleads to substantial </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">improvements over a conventional RAG baseline for both the\\ncomprehensiveness and diversity of generated answers.'</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2025-02-19'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'From Local to Global: A Graph RAG Approach to Query-Focused Summarization'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Dasha \u001b[0m\n",
              "\u001b[32mMetropolitansky, Robert Osazuwa Ness, Jonathan Larson'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'The use of retrieval-augmented generation \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m to retrieve relevant\\ninformation from an external \u001b[0m\n",
              "\u001b[32mknowledge source enables large language models\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m to answer questions over private and/or previously unseen \u001b[0m\n",
              "\u001b[32mdocument\\ncollections. However, RAG fails on global questions directed at an entire text\\ncorpus, such as \"What are\u001b[0m\n",
              "\u001b[32mthe main themes in the dataset?\", since this is\\ninherently a query-focused summarization \u001b[0m\u001b[32m(\u001b[0m\u001b[32mQFS\u001b[0m\u001b[32m)\u001b[0m\u001b[32m task, rather than \u001b[0m\n",
              "\u001b[32man explicit\\nretrieval task. Prior QFS methods, meanwhile, do not scale to the quantities of\\ntext indexed by \u001b[0m\n",
              "\u001b[32mtypical RAG systems. To combine the strengths of these\\ncontrasting methods, we propose GraphRAG, a graph-based \u001b[0m\n",
              "\u001b[32mapproach to question\\nanswering over private text corpora that scales with both the generality of\\nuser questions \u001b[0m\n",
              "\u001b[32mand the quantity of source text. Our approach uses an LLM to\\nbuild a graph index in two stages: first, to derive \u001b[0m\n",
              "\u001b[32man entity knowledge graph\\nfrom the source documents, then to pregenerate community summaries for all\\ngroups of \u001b[0m\n",
              "\u001b[32mclosely related entities. Given a question, each community summary is\\nused to generate a partial response, before \u001b[0m\n",
              "\u001b[32mall partial responses are again\\nsummarized in a final response to the user. For a class of global \u001b[0m\n",
              "\u001b[32msensemaking\\nquestions over datasets in the 1 million token range, we show that GraphRAG\\nleads to substantial \u001b[0m\n",
              "\u001b[32mimprovements over a conventional RAG baseline for both the\\ncomprehensiveness and diversity of generated answers.'\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "pprint(documents[0].metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSPkNPBLvbMe"
      },
      "source": [
        "# 3. Transforming the documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ntp3-wFvxtd"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 1200,\n",
        "    chunk_overlap  = 200,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \";\", \",\", \" \", \"\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4eGAQSs7vxrH"
      },
      "outputs": [],
      "source": [
        "docs_split = text_splitter.split_documents(documents)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d2Tw2GDJRk9"
      },
      "source": [
        "The code filters out documents that are mostly filled with numbers or irrelevant content, ensuring that only those documents with a significant amount of meaningful (alphabetic) text remain in the `docs_split` list.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MrpHKM6vxpA",
        "outputId": "d0867a46-7a62-4ae9-d232-c027807c5159"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "86\n"
          ]
        }
      ],
      "source": [
        " def include_doc(doc):\n",
        "     ## Some chunks will be overburdened with useless numerical data, so we'll filter it out\n",
        "     string = doc.page_content\n",
        "     if len([l for l in string if l.isalpha()]) < (len(string)//2):\n",
        "         return False\n",
        "     return True\n",
        "\n",
        "docs_split = [doc for doc in docs_split if include_doc(doc)]\n",
        "print(len(docs_split))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7tA6uQ05vxmx",
        "outputId": "b3298b19-8fa8-4c81-8443-a21febdae942"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mDocument \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "From Local to Global: A GraphRAG Approach to\n",
            "Query-Focused Summarization\n",
            "Darren Edge1†\n",
            "Ha Trinh1†\n",
            "Newman Cheng2\n",
            "Joshua Bradley2\n",
            "Alex Chao3\n",
            "Apurva Mody3\n",
            "Steven Truitt2\n",
            "Dasha Metropolitansky1\n",
            "Robert Osazuwa Ness1\n",
            "Jonathan Larson1\n",
            "1Microsoft Research\n",
            "2Microsoft Strategic Missions and Technologies\n",
            "3Microsoft Office of the CTO\n",
            "{daedge,trinhha,newmancheng,joshbradley,achao,moapurva,\n",
            "steventruitt,dasham,robertness,jolarso}@microsoft.com\n",
            "†These authors contributed equally to this work\n",
            "Abstract\n",
            "The use of retrieval-augmented generation (RAG) to retrieve relevant informa-\n",
            "tion from an external knowledge source enables large language models (LLMs)\n",
            "to answer questions over private and/or previously unseen document collections.\n",
            "However, RAG fails on global questions directed at an entire text corpus, such\n",
            "as “What are the main themes in the dataset?”, since this is inherently a query-\n",
            "focused summarization (QFS) task, rather than an explicit retrieval task. Prior\n",
            "QFS methods, meanwhile, do not scale to the quantities of text indexed by typ-\n",
            "ical RAG systems. To combine the strengths of these contrasting methods, we\n",
            "propose GraphRAG, a graph-based approach to question answering over private\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">================================================================</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;118;185;0m================================================================\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mDocument \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ical RAG systems. To combine the strengths of these contrasting methods, we\n",
            "propose GraphRAG, a graph-based approach to question answering over private\n",
            "text corpora that scales with both the generality of user questions and the quantity\n",
            "of source text. Our approach uses an LLM to build a graph index in two stages:\n",
            "first, to derive an entity knowledge graph from the source documents, then to pre-\n",
            "generate community summaries for all groups of closely related entities. Given a\n",
            "question, each community summary is used to generate a partial response, before\n",
            "all partial responses are again summarized in a final response to the user. For a\n",
            "class of global sensemaking questions over datasets in the 1 million token range,\n",
            "we show that GraphRAG leads to substantial improvements over a conventional\n",
            "RAG baseline for both the comprehensiveness and diversity of generated answers.\n",
            "1\n",
            "Introduction\n",
            "Retrieval augmented generation (RAG) (Lewis et al., 2020) is an established approach to using\n",
            "LLMs to answer queries based on data that is too large to contain in a language model’s context\n",
            "window, meaning the maximum number of tokens (units of text) that can be processed by the LLM\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">================================================================</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;118;185;0m================================================================\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mDocument \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLMs to answer queries based on data that is too large to contain in a language model’s context\n",
            "window, meaning the maximum number of tokens (units of text) that can be processed by the LLM\n",
            "at once (Kuratov et al., 2024; Liu et al., 2023). In the canonical RAG setup, the system has access to\n",
            "a large external corpus of text records and retrieves a subset of records that are individually relevant\n",
            "to the query and collectively small enough to fit into the context window of the LLM. The LLM then\n",
            "Preprint. Under review.\n",
            "arXiv:2404.16130v2  [cs.CL]  19 Feb 2025\n",
            "generates a response based on both the query and the retrieved records (Baumel et al., 2018; Dang,\n",
            "2006; Laskar et al., 2020; Yao et al., 2017). This conventional approach, which we collectively call\n",
            "vector RAG, works well for queries that can be answered with information localized within a small\n",
            "set of records. However, vector RAG approaches do not support sensemaking queries, meaning\n",
            "queries that require global understanding of the entire dataset, such as ”What are the key trends in\n",
            "how scientific discoveries are influenced by interdisciplinary research over the past decade?”\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">================================================================</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;118;185;0m================================================================\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mDocument \u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3.1.2\n",
            "Text Chunks →Entities & Relationships\n",
            "In this step, the LLM is prompted to extract instances of important entities and the relationships\n",
            "between the entities from a given chunk. Additionally, the LLM generates short descriptions for the\n",
            "entities and relationships. To illustrate, suppose a chunk contained the following text:\n",
            "4\n",
            "NeoChip’s (NC) shares surged in their first week of trading on the NewTech Ex-\n",
            "change. However, market analysts caution that the chipmaker’s public debut may\n",
            "not reflect trends for other technology IPOs. NeoChip, previously a private entity,\n",
            "was acquired by Quantum Systems in 2016. The innovative semiconductor firm\n",
            "specializes in low-power processors for wearables and IoT devices.\n",
            "The LLM is prompted such that it extracts the following:\n",
            "• The entity NeoChip, with description “NeoChip is a publicly traded company specializing\n",
            "in low-power processors for wearables and IoT devices.”\n",
            "• The entity Quantum Systems, with description “Quantum Systems is a firm that previ-\n",
            "ously owned NeoChip.”\n",
            "• A relationship between NeoChip and Quantum Systems, with description “Quantum Sys-\n",
            "tems owned NeoChip from 2016 until NeoChip became publicly traded.”\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">================================================================</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;118;185;0m================================================================\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mDocument \u001b[0m\u001b[1;36m-1\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For example, if the question is ’What is the capital\n",
            "of France?’, a direct answer would be ’Paris’.\n",
            "A direct answer should not provide any irrelevant or\n",
            "unnecessary information that does not answer the question.\n",
            "For example, an indirect answer would be ’The\n",
            "capital of France is located on the river Seine’.\",\n",
            "\"empowerment\":\n",
            "\"How well does the answer help the reader understand and make informed judgements about\n",
            "the topic without being misled or making fallacious assumptions.\n",
            "Evaluate each answer on the quality of\n",
            "answer as it relates to clearly explaining and providing reasoning and sources behind the claims in the\n",
            "answer.\"\n",
            "}\n",
            "25\n",
            "G\n",
            "Statistical Analysis\n",
            "Table 6: Pairwise comparisons of six conditions on four metrics across 125 questions and two\n",
            "datasets. For each question and metric, the winning condition received a score of 100, the losing\n",
            "condition received a score of 0, and in the event of a tie, each condition was scored 50. These scores\n",
            "were then averaged over five evaluation runs for each condition. Results of Shapiro-Wilk tests in-\n",
            "dicated that the data did not follow a normal distribution. Thus, non-parametric tests (Wilcoxon\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">================================================================</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;118;185;0m================================================================\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "for i in (0, 1, 2, 15, -1):\n",
        "    pprint(f\"[Document {i}]\")\n",
        "    print(docs_split[i].page_content)\n",
        "    pprint(\"=\"*64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhV0nVK6wizf"
      },
      "source": [
        "# 4. Refining summaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWN64EOswzWk"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain_core.runnables.passthrough import RunnableAssign\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.output_parsers import PydanticOutputParser\n",
        "\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "from IPython.display import clear_output\n",
        "\n",
        "class DocummentSummaryBase(BaseModel):\n",
        "  running_summary: str = Field(\"\", description='Running description of the document. Do not override; only update!')\n",
        "  main_ideas: List[str] = Field([], description='Most important information from the document (max 3)')\n",
        "  loose_ends: List[str] = Field([], description=\"Open questions that would be good to incorporate into summary, but that are yet unknown (max 3)\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSLhGOxEwzTN"
      },
      "outputs": [],
      "source": [
        "# prompt\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(\n",
        "    \"You are generating a running summary of the document. Make it readable by a technical user.\"\n",
        "    \" After this, the old knowledge base will be replaced by the new one. Make sure a reader can still understand everything.\"\n",
        "    \" Keep it short, but as dense and useful as possible! The information should flow from chunk to (loose ends or main ideas) to running_summary.\"\n",
        "    \" The updated knowledge base keep all of the information from running_summary here: {info_base}.\"\n",
        "    \"\\n\\n{format_instructions}. Follow the format precisely, including quotations and commas\"\n",
        "    \"\\n\\nWithout losing any of the info, update the knowledge base with the following: {input}\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOJ6h2-nKbTn"
      },
      "source": [
        "\n",
        "The `reextract` function performs a series of operations to:\n",
        "- Extract data using an LLM and a prompt.\n",
        "- Format and parse the data according to a given `pydantic_class`.\n",
        "- Return the parsed data in a structured dictionary format, with the key `'info_base'`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKfr05sCwzQ2"
      },
      "outputs": [],
      "source": [
        "def reextract(pydantic_class, llm, prompt):\n",
        "    '''\n",
        "    Runnable Extraction module\n",
        "    Returns a knowledge dictionary populated by slot-filling extraction\n",
        "    '''\n",
        "    parser = PydanticOutputParser(pydantic_object=pydantic_class)\n",
        "    instruct_merge = RunnableAssign({'format_instructions' : lambda x: parser.get_format_instructions()})\n",
        "    def preparse(string):\n",
        "       if '{' not in string: string = '{' + string\n",
        "       if '}' not in string: string = string + '}'\n",
        "       string = (string\n",
        "            .replace(\"\\\\_\", \"_\")a\n",
        "            .replace(\"\\n\", \" \")\n",
        "            .replace(\"\\]\", \"]\")\n",
        "            .replace(\"\\[\", \"[\")\n",
        "        )\n",
        "        # print(string)  ## Good for diagnostics\n",
        "       return string\n",
        "    # Return a dictionary with 'info_base' containing the parsed output\n",
        "    return instruct_merge | prompt | llm | preparse | parser | RunnableLambda(lambda x: {'info_base': x})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "GCbn3XwawzOm",
        "outputId": "a77cf844-5a99-4c2b-dfc0-4ecab132586d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Considered 15 documents\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DocummentSummaryBase</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">running_summary</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'GraphRAG is a novel graph-based approach for Query-Focused Summarization (QFS) that integrates</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Retrieval-Augmented Generation (RAG) and graph-based techniques. It allows Language Learning Models (LLMs) to </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">handle large datasets and enhances the comprehensiveness and diversity of generated answers. GraphRAG operates in </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">two stages: creating an entity knowledge graph and pre-generating community summaries using an LLM. It constructs a</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">knowledge graph using an LLM, with nodes representing key entities and edges representing relationships between </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">them. The graph is partitioned into hierarchical communities of related entities, where community summaries are </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">generated and aggregated into global summaries. Adaptive benchmarking for RAG Evaluation is also proposed, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">introducing an approach for generating a set of questions for evaluating global sensemaking over the entirety of </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">the corpus. Community detection (e.g., Leiden, Traag et al., 2019) is used to partition the graph index into groups</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">of elements (nodes, edges, covariates) that the LLM can summarize in parallel at both indexing time and query time.</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">The ‘global answer’ to a given query is produced using a final round of query-focused summarization over all </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">community summaries reporting relevance to that query. In this work, we design criteria for evaluating </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">RAG-generated answers to global sensemaking questions and evaluate our results using the comparative approach. We </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">also validate results using statistics derived from LLM-extracted statements of verifiable facts, or ‘claims.’ The </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Methods section further details the GraphRAG workflow, including the extraction of important entities and </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">relationships from text chunks, the formation of the knowledge graph, community detection, summary generation, and </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">global answer production.'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">main_ideas</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'GraphRAG is a graph-based approach to question answering over large text corpora'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'GraphRAG combines the strengths of RAG and graph-based methods'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'The main contribution of this work is GraphRAG’s ability to perform global sensemaking over an entire </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">corpus'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'Adaptive benchmarking is introduced for generating questions to assess global sensemaking'</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ],</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">loose_ends</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'How does GraphRAG differ from traditional RAG and QFS methods in terms of performance and efficiency?'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'What specific types of global sensemaking questions can be answered using GraphRAG?'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'How does GraphRAG compare to other versions of the GraphRAG approach available as extensions to </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">open-source libraries?'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'How does GraphRAG compare to other techniques that use subgraphs or properties of the graph structure </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">directly in the prompt?'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'How does GraphRAG compare to techniques that use the knowledge graph to enhance retrieval?'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'What are the recent works that have used LLMs for adaptive benchmarking?'</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ]</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">)</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;35mDocummentSummaryBase\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mrunning_summary\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'GraphRAG is a novel graph-based approach for Query-Focused Summarization \u001b[0m\u001b[32m(\u001b[0m\u001b[32mQFS\u001b[0m\u001b[32m)\u001b[0m\u001b[32m that integrates\u001b[0m\n",
              "\u001b[32mRetrieval-Augmented Generation \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and graph-based techniques. It allows Language Learning Models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m to \u001b[0m\n",
              "\u001b[32mhandle large datasets and enhances the comprehensiveness and diversity of generated answers. GraphRAG operates in \u001b[0m\n",
              "\u001b[32mtwo stages: creating an entity knowledge graph and pre-generating community summaries using an LLM. It constructs a\u001b[0m\n",
              "\u001b[32mknowledge graph using an LLM, with nodes representing key entities and edges representing relationships between \u001b[0m\n",
              "\u001b[32mthem. The graph is partitioned into hierarchical communities of related entities, where community summaries are \u001b[0m\n",
              "\u001b[32mgenerated and aggregated into global summaries. Adaptive benchmarking for RAG Evaluation is also proposed, \u001b[0m\n",
              "\u001b[32mintroducing an approach for generating a set of questions for evaluating global sensemaking over the entirety of \u001b[0m\n",
              "\u001b[32mthe corpus. Community detection \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., Leiden, Traag et al., 2019\u001b[0m\u001b[32m)\u001b[0m\u001b[32m is used to partition the graph index into groups\u001b[0m\n",
              "\u001b[32mof elements \u001b[0m\u001b[32m(\u001b[0m\u001b[32mnodes, edges, covariates\u001b[0m\u001b[32m)\u001b[0m\u001b[32m that the LLM can summarize in parallel at both indexing time and query time.\u001b[0m\n",
              "\u001b[32mThe ‘global answer’ to a given query is produced using a final round of query-focused summarization over all \u001b[0m\n",
              "\u001b[32mcommunity summaries reporting relevance to that query. In this work, we design criteria for evaluating \u001b[0m\n",
              "\u001b[32mRAG-generated answers to global sensemaking questions and evaluate our results using the comparative approach. We \u001b[0m\n",
              "\u001b[32malso validate results using statistics derived from LLM-extracted statements of verifiable facts, or ‘claims.’ The \u001b[0m\n",
              "\u001b[32mMethods section further details the GraphRAG workflow, including the extraction of important entities and \u001b[0m\n",
              "\u001b[32mrelationships from text chunks, the formation of the knowledge graph, community detection, summary generation, and \u001b[0m\n",
              "\u001b[32mglobal answer production.'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mmain_ideas\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'GraphRAG is a graph-based approach to question answering over large text corpora'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'GraphRAG combines the strengths of RAG and graph-based methods'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'The main contribution of this work is GraphRAG’s ability to perform global sensemaking over an entire \u001b[0m\n",
              "\u001b[32mcorpus'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'Adaptive benchmarking is introduced for generating questions to assess global sensemaking'\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mloose_ends\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'How does GraphRAG differ from traditional RAG and QFS methods in terms of performance and efficiency?'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'What specific types of global sensemaking questions can be answered using GraphRAG?'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'How does GraphRAG compare to other versions of the GraphRAG approach available as extensions to \u001b[0m\n",
              "\u001b[32mopen-source libraries?'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'How does GraphRAG compare to other techniques that use subgraphs or properties of the graph structure \u001b[0m\n",
              "\u001b[32mdirectly in the prompt?'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'How does GraphRAG compare to techniques that use the knowledge graph to enhance retrieval?'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'What are the recent works that have used LLMs for adaptive benchmarking?'\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m)\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "latest_summary = \"\"\n",
        "\n",
        "def RSummarizer(knowledge, llm, prompt, verbose=False):\n",
        "    '''\n",
        "    Exercise: Create a chain that summarizes\n",
        "    '''\n",
        "\n",
        "    def summarize_docs(docs):\n",
        "\n",
        "        parse_chain = reextract(knowledge.__class__, llm, prompt)\n",
        "\n",
        "        state = {'info_base': knowledge}\n",
        "\n",
        "        global latest_summary\n",
        "        for i, doc in enumerate(docs):\n",
        "            state = parse_chain.invoke({'input': doc.page_content, 'info_base': state['info_base']})\n",
        "\n",
        "            assert 'info_base' in state\n",
        "            if verbose:\n",
        "                print(f\"Considered {i+1} documents\")\n",
        "                pprint(state['info_base'])\n",
        "                latest_summary = state['info_base']\n",
        "                clear_output(wait=True)\n",
        "\n",
        "        return state['info_base']\n",
        "\n",
        "    return RunnableLambda(summarize_docs)\n",
        "\n",
        "instruct_model = ChatNVIDIA(model=\"mistralai/mixtral-8x22b-instruct-v0.1\").bind(max_tokens=4096)\n",
        "instruct_llm = instruct_model | StrOutputParser()\n",
        "\n",
        "## Take the first 10 document chunks and accumulate a DocumentSummaryBase\n",
        "summarizer = RSummarizer(DocummentSummaryBase(), instruct_llm, prompt_template, verbose=True) # This line was problematic\n",
        "summary = summarizer.invoke(docs_split[:15])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593
        },
        "id": "S_VljXghwzMZ",
        "outputId": "85c961cc-88d5-4e16-923e-226521e035cd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DocummentSummaryBase</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">running_summary</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'GraphRAG is a novel graph-based approach for Query-Focused Summarization (QFS) that integrates</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Retrieval-Augmented Generation (RAG) and graph-based techniques. It allows Language Learning Models (LLMs) to </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">handle large datasets and enhances the comprehensiveness and diversity of generated answers. GraphRAG operates in </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">two stages: creating an entity knowledge graph and pre-generating community summaries using an LLM. It constructs a</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">knowledge graph using an LLM, with nodes representing key entities and edges representing relationships between </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">them. The graph is partitioned into hierarchical communities of related entities, where community summaries are </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">generated and aggregated into global summaries. Adaptive benchmarking for RAG Evaluation is also proposed, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">introducing an approach for generating a set of questions for evaluating global sensemaking over the entirety of </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">the corpus. Community detection (e.g., Leiden, Traag et al., 2019) is used to partition the graph index into groups</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">of elements (nodes, edges, covariates) that the LLM can summarize in parallel at both indexing time and query time.</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">The ‘global answer’ to a given query is produced using a final round of query-focused summarization over all </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">community summaries reporting relevance to that query. In this work, we design criteria for evaluating </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">RAG-generated answers to global sensemaking questions and evaluate our results using the comparative approach. We </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">also validate results using statistics derived from LLM-extracted statements of verifiable facts, or ‘claims.’ The </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Methods section further details the GraphRAG workflow, including the extraction of important entities and </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">relationships from text chunks, the formation of the knowledge graph, community detection, summary generation, and </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">global answer production.'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">main_ideas</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'GraphRAG is a graph-based approach to question answering over large text corpora'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'GraphRAG combines the strengths of RAG and graph-based methods'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'The main contribution of this work is GraphRAG’s ability to perform global sensemaking over an entire </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">corpus'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'Adaptive benchmarking is introduced for generating questions to assess global sensemaking'</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ],</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">loose_ends</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'How does GraphRAG differ from traditional RAG and QFS methods in terms of performance and efficiency?'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'What specific types of global sensemaking questions can be answered using GraphRAG?'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'How does GraphRAG compare to other versions of the GraphRAG approach available as extensions to </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">open-source libraries?'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'How does GraphRAG compare to other techniques that use subgraphs or properties of the graph structure </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">directly in the prompt?'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'How does GraphRAG compare to techniques that use the knowledge graph to enhance retrieval?'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #008000; text-decoration-color: #008000\">'What are the recent works that have used LLMs for adaptive benchmarking?'</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ]</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">)</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;35mDocummentSummaryBase\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mrunning_summary\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'GraphRAG is a novel graph-based approach for Query-Focused Summarization \u001b[0m\u001b[32m(\u001b[0m\u001b[32mQFS\u001b[0m\u001b[32m)\u001b[0m\u001b[32m that integrates\u001b[0m\n",
              "\u001b[32mRetrieval-Augmented Generation \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and graph-based techniques. It allows Language Learning Models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m to \u001b[0m\n",
              "\u001b[32mhandle large datasets and enhances the comprehensiveness and diversity of generated answers. GraphRAG operates in \u001b[0m\n",
              "\u001b[32mtwo stages: creating an entity knowledge graph and pre-generating community summaries using an LLM. It constructs a\u001b[0m\n",
              "\u001b[32mknowledge graph using an LLM, with nodes representing key entities and edges representing relationships between \u001b[0m\n",
              "\u001b[32mthem. The graph is partitioned into hierarchical communities of related entities, where community summaries are \u001b[0m\n",
              "\u001b[32mgenerated and aggregated into global summaries. Adaptive benchmarking for RAG Evaluation is also proposed, \u001b[0m\n",
              "\u001b[32mintroducing an approach for generating a set of questions for evaluating global sensemaking over the entirety of \u001b[0m\n",
              "\u001b[32mthe corpus. Community detection \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., Leiden, Traag et al., 2019\u001b[0m\u001b[32m)\u001b[0m\u001b[32m is used to partition the graph index into groups\u001b[0m\n",
              "\u001b[32mof elements \u001b[0m\u001b[32m(\u001b[0m\u001b[32mnodes, edges, covariates\u001b[0m\u001b[32m)\u001b[0m\u001b[32m that the LLM can summarize in parallel at both indexing time and query time.\u001b[0m\n",
              "\u001b[32mThe ‘global answer’ to a given query is produced using a final round of query-focused summarization over all \u001b[0m\n",
              "\u001b[32mcommunity summaries reporting relevance to that query. In this work, we design criteria for evaluating \u001b[0m\n",
              "\u001b[32mRAG-generated answers to global sensemaking questions and evaluate our results using the comparative approach. We \u001b[0m\n",
              "\u001b[32malso validate results using statistics derived from LLM-extracted statements of verifiable facts, or ‘claims.’ The \u001b[0m\n",
              "\u001b[32mMethods section further details the GraphRAG workflow, including the extraction of important entities and \u001b[0m\n",
              "\u001b[32mrelationships from text chunks, the formation of the knowledge graph, community detection, summary generation, and \u001b[0m\n",
              "\u001b[32mglobal answer production.'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mmain_ideas\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'GraphRAG is a graph-based approach to question answering over large text corpora'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'GraphRAG combines the strengths of RAG and graph-based methods'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'The main contribution of this work is GraphRAG’s ability to perform global sensemaking over an entire \u001b[0m\n",
              "\u001b[32mcorpus'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'Adaptive benchmarking is introduced for generating questions to assess global sensemaking'\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mloose_ends\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'How does GraphRAG differ from traditional RAG and QFS methods in terms of performance and efficiency?'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'What specific types of global sensemaking questions can be answered using GraphRAG?'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'How does GraphRAG compare to other versions of the GraphRAG approach available as extensions to \u001b[0m\n",
              "\u001b[32mopen-source libraries?'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'How does GraphRAG compare to other techniques that use subgraphs or properties of the graph structure \u001b[0m\n",
              "\u001b[32mdirectly in the prompt?'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'How does GraphRAG compare to techniques that use the knowledge graph to enhance retrieval?'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[32m'What are the recent works that have used LLMs for adaptive benchmarking?'\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m)\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "pprint(latest_summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sg5MqQn4kMg"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
